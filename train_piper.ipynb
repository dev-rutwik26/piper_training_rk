{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piper TTS Fine-Tuning (2025 Compatible)\n",
    "\n",
    "This notebook allows you to fine-tune a Piper TTS model using your own voice. It uses the maintained `OHF-Voice` fork to ensure compatibility after the archiving of the original repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "Install necessary dependencies and the Piper training tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install system dependencies\n",
    "!sudo apt-get install -y espeak-ng\n",
    "\n",
    "# Clone the maintained fork of Piper\n",
    "!git clone https://github.com/OHF-Voice/piper1-gpl.git\n",
    "%cd piper1-gpl/src/python\n",
    "\n",
    "# Install python dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install -e .[train]\n",
    "\n",
    "# Build the monotonic alignment search (required for training)\n",
    "!./build_monotonic_align.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive\n",
    "Mount your drive to access your dataset and save your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# CONFIGURATION - Update these paths!\n",
    "dataset_zip_path = \"/content/drive/MyDrive/piper_training/my_voice_dataset.zip\"\n",
    "model_name = \"my_custom_voice\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset\n",
    "Unzip your dataset. The zip file should contain a `metadata.csv` and a folder of wav files, formatted according to LJSpeech standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q \"$dataset_zip_path\" -d /content/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing\n",
    "Download a base model to fine-tune from (e.g., typically `en_US-lessac-medium` or similar) and preprocess your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for training artifacts\n",
    "!mkdir -p /content/training\n",
    "\n",
    "# Preprocess the dataset\n",
    "# Adjust --language and --sample-rate (22050 for medium/high, 16000 for low) as needed\n",
    "!python3 -m piper_train.preprocess \\\n",
    "  --language en \\\n",
    "  --input-dir /content/dataset \\\n",
    "  --output-dir /content/training \\\n",
    "  --dataset-format ljspeech \\\n",
    "  --single-speaker \\\n",
    "  --sample-rate 22050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training (Fine-Tuning)\n",
    "Start the training process. \n",
    "**Note:** You need a base checkpoint to resume from. You can download one using `wget`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Download en_US-libritts_r-medium checkpoint (adjust URL for your desired base model)\n",
    "!wget -O /content/training/epoch=2324-step=1355936.ckpt https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/libritts_r/medium/epoch%3D2324-step%3D1355936.ckpt\n",
    "\n",
    "# Start Fine-Tuning\n",
    "!python3 -m piper_train \\\n",
    "  --dataset-dir /content/training \\\n",
    "  --accelerator gpu \\\n",
    "  --devices 1 \\\n",
    "  --batch-size 32 \\\n",
    "  --validation-split 0.0 \\\n",
    "  --num-test-examples 0 \\\n",
    "  --max-epochs 6000 \\\n",
    "  --resume_from_checkpoint /content/training/epoch=2324-step=1355936.ckpt \\\n",
    "  --checkpoint-epochs 50 \\\n",
    "  --precision 16-mixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Model\n",
    "Convert the trained checkpoint to ONNX format for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX\n",
    "!python3 -m piper_train.export_onnx \\\n",
    "  /content/training/lightning_logs/version_0/checkpoints/*.ckpt \\\n",
    "  /content/drive/MyDrive/piper_training/$model_name.onnx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Piper TTS Training on AWS SageMaker\n",
                "\n",
                "This notebook is adapted for AWS SageMaker (Notebook Instances or Studio). It handles:\n",
                "1.  **OS Detection**: Automatically uses `yum` (Amazon Linux) or `apt-get` (Ubuntu) for system dependencies.\n",
                "2.  **Persistent Storage**: Builds dependencies in the local workspace so they survive kernel restarts (if using persistent storage).\n",
                "3.  **Piper-Phonemize**: Building from source to resolve compatibility issues."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Environment\n",
                "We determine the OS and install necessary build tools."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üõ†Ô∏è System Dependencies & OS Check\n",
                "import subprocess\n",
                "import os\n",
                "import sys\n",
                "\n",
                "def install_system_deps():\n",
                "    print(\"üîç Checking OS...\")\n",
                "    if os.path.exists(\"/usr/bin/yum\"):\n",
                "        print(\"üì¶ Amazon Linux detected. Using yum...\")\n",
                "        # Amazon Linux deps\n",
                "        subprocess.check_call([\"sudo\", \"yum\", \"groupinstall\", \"-y\", \"Development Tools\"])\n",
                "        subprocess.check_call([\"sudo\", \"yum\", \"install\", \"-y\", \"cmake\", \"git\", \"libtool\", \"automake\", \"autoconf\"])\n",
                "    elif os.path.exists(\"/usr/bin/apt-get\"):\n",
                "        print(\"üì¶ Ubuntu/Debian detected. Using apt-get...\")\n",
                "        # Ubuntu deps\n",
                "        subprocess.check_call([\"sudo\", \"apt-get\", \"update\", \"-qq\"])\n",
                "        subprocess.check_call([\"sudo\", \"apt-get\", \"install\", \"-y\", \"-qq\", \"build-essential\", \"cmake\", \"git\", \"autoconf\", \"automake\", \"libtool\", \"pkg-config\"])\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è Unknown OS. Please install build-essential, cmake, git, automake manually.\")\n",
                "\n",
                "install_system_deps()\n",
                "print(\"‚úÖ System dependencies installed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üõ†Ô∏è Build and Install Piper Dependencies\n",
                "import os\n",
                "import sys\n",
                "import shutil\n",
                "\n",
                "# Define a workspace directory (current dir is usually persistent in SageMaker)\n",
                "WORKSPACE_DIR = os.getcwd()\n",
                "BUILD_DIR = os.path.join(WORKSPACE_DIR, \"build_temp\")\n",
                "os.makedirs(BUILD_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"üöÄ Building in: {BUILD_DIR}\")\n",
                "\n",
                "# 1. Build & Install Rhasspy's espeak-ng Fork\n",
                "if not os.path.exists(os.path.join(BUILD_DIR, \"espeak-ng\")):\n",
                "    print(\"\\n‚¨áÔ∏è Cloning rhasspy/espeak-ng...\")\n",
                "    !cd {BUILD_DIR} && git clone https://github.com/rhasspy/espeak-ng.git\n",
                "\n",
                "print(\"\\nüî® Building espeak-ng... (This may take a few minutes)\")\n",
                "# Note: In SageMaker we can install to /usr/local if we have sudo, or a local prefix.\n",
                "# We'll try /usr first as it simplifies linking.\n",
                "!cd {BUILD_DIR}/espeak-ng && ./autogen.sh && ./configure --prefix=/usr --without-async --without-mbrola --without-sonic && make -j4 && sudo make install\n",
                "print(\"‚úÖ espeak-ng installed!\")\n",
                "\n",
                "# 2. Setup Python Build Environment\n",
                "print(\"\\nüêç Setting up Python environment...\")\n",
                "!pip install -q cython numpy pybind11 setuptools wheel\n",
                "\n",
                "# 3. Custom Build of piper-phonemize\n",
                "if os.path.exists(os.path.join(BUILD_DIR, \"piper-phonemize\")):\n",
                "    shutil.rmtree(os.path.join(BUILD_DIR, \"piper-phonemize\"))\n",
                "\n",
                "!cd {BUILD_DIR} && git clone https://github.com/rhasspy/piper-phonemize.git\n",
                "\n",
                "# Download ONNX Runtime (needed for headers)\n",
                "print(\"\\n‚¨áÔ∏è Downloading ONNX Runtime...\")\n",
                "onnx_ver = \"1.14.1\"\n",
                "onnx_file = f\"onnxruntime-linux-x64-{onnx_ver}.tgz\"\n",
                "!cd {BUILD_DIR}/piper-phonemize && wget -q https://github.com/microsoft/onnxruntime/releases/download/v{onnx_ver}/{onnx_file}\n",
                "!cd {BUILD_DIR}/piper-phonemize && tar -xzf {onnx_file}\n",
                "!mkdir -p {BUILD_DIR}/piper-phonemize/lib/Linux-x86_64/onnxruntime/include\n",
                "!mkdir -p {BUILD_DIR}/piper-phonemize/lib/Linux-x86_64/onnxruntime/lib\n",
                "!cp -r {BUILD_DIR}/piper-phonemize/onnxruntime-linux-x64-{onnx_ver}/include/* {BUILD_DIR}/piper-phonemize/lib/Linux-x86_64/onnxruntime/include/\n",
                "!cp -r {BUILD_DIR}/piper-phonemize/onnxruntime-linux-x64-{onnx_ver}/lib/* {BUILD_DIR}/piper-phonemize/lib/Linux-x86_64/onnxruntime/lib/\n",
                "\n",
                "# Patch setup.py\n",
                "print(\"\\nüîß Patching setup.py...\")\n",
                "# Added /usr/local/include to include_dirs to find espeak-ng headers if installed there\n",
                "setup_content = \"\"\"\n",
                "import platform\n",
                "from pathlib import Path\n",
                "from pybind11.setup_helpers import Pybind11Extension, build_ext\n",
                "from setuptools import setup\n",
                "\n",
                "_DIR = Path(__file__).parent\n",
                "_ONNXRUNTIME_DIR = _DIR / \"lib\" / f\"Linux-{platform.machine()}\" / \"onnxruntime\"\n",
                "\n",
                "__version__ = \"1.2.0\"\n",
                "\n",
                "ext_modules = [\n",
                "    Pybind11Extension(\n",
                "        \"piper_phonemize_cpp\",\n",
                "        [\n",
                "            \"src/python.cpp\",\n",
                "            \"src/phonemize.cpp\",\n",
                "            \"src/phoneme_ids.cpp\",\n",
                "            \"src/tashkeel.cpp\",\n",
                "        ],\n",
                "        define_macros=[(\"VERSION_INFO\", __version__)],\n",
                "        include_dirs=[\"/usr/include\", \"/usr/local/include\", str(_ONNXRUNTIME_DIR / \"include\")],\n",
                "        library_dirs=[\"/usr/lib\", \"/usr/local/lib\", str(_ONNXRUNTIME_DIR / \"lib\")],\n",
                "        libraries=[\"espeak-ng\", \"onnxruntime\"],\n",
                "    ),\n",
                "]\n",
                "\n",
                "setup(\n",
                "    name=\"piper_phonemize\",\n",
                "    version=__version__,\n",
                "    packages=[\"piper_phonemize\"],\n",
                "    package_data={\n",
                "        \"piper_phonemize\": [\n",
                "            str(p) for p in (_DIR / \"piper_phonemize\" / \"espeak-ng-data\").rglob(\"*\")\n",
                "        ] + [str(_DIR / \"libtashkeel_model.ort\")]\n",
                "    },\n",
                "    include_package_data=True,\n",
                "    ext_modules=ext_modules,\n",
                "    cmdclass={\"build_ext\": build_ext},\n",
                "    zip_safe=False,\n",
                ")\n",
                "\"\"\"\n",
                "with open(os.path.join(BUILD_DIR, \"piper-phonemize\", \"setup.py\"), \"w\") as f:\n",
                "    f.write(setup_content)\n",
                "\n",
                "# Build and Install\n",
                "print(\"\\nüêç Compiling python extension...\")\n",
                "!cd {BUILD_DIR}/piper-phonemize && python setup.py build_ext --inplace\n",
                "!cd {BUILD_DIR}/piper-phonemize && python setup.py install\n",
                "\n",
                "print(\"\\n‚úÖ Build Process Complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üöë Runtime Fix: Link Libraries\n",
                "import sys\n",
                "import glob\n",
                "import shutil\n",
                "import os\n",
                "\n",
                "BUILD_DIR = os.path.join(os.getcwd(), \"build_temp\")\n",
                "print(\"üîß Applying Runtime Fixes...\")\n",
                "\n",
                "# 1. Copy missing ONNX Runtime library to /usr/lib\n",
                "src_lib = os.path.join(BUILD_DIR, \"piper-phonemize/lib/Linux-x86_64/onnxruntime/lib/libonnxruntime.so.1.14.1\")\n",
                "if os.path.exists(src_lib):\n",
                "    try:\n",
                "        # Try copying to /usr/lib if we have permission (sudo)\n",
                "        !sudo cp {src_lib} /usr/lib/libonnxruntime.so.1.14.1\n",
                "        !sudo ln -fs /usr/lib/libonnxruntime.so.1.14.1 /usr/lib/libonnxruntime.so\n",
                "        !sudo ldconfig\n",
                "        print(\"‚úÖ Copied libonnxruntime.so to /usr/lib\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Cannot copy to /usr/lib ({e}). Trying LD_LIBRARY_PATH approach...\")\n",
                "        # Fallback for non-sudo environments: Add to LD_LIBRARY_PATH (requires restart usually, or runtime hack)\n",
                "        lib_dir = os.path.dirname(src_lib)\n",
                "        os.environ[\"LD_LIBRARY_PATH\"] = f\"{lib_dir}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
                "        print(f\"‚ö†Ô∏è Added {lib_dir} to LD_LIBRARY_PATH. You might need to restart kernel if this fails.\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Warning: Could not find downloaded ONNX runtime source.\")\n",
                "\n",
                "# 2. Find and add the installed egg to python path if needed\n",
                "import site\n",
                "# Reload site packages to find newly installed egg\n",
                "from importlib import reload\n",
                "reload(site)\n",
                "\n",
                "try:\n",
                "    import piper_phonemize\n",
                "    print(\"\\nüéâ SUCCESS: piper_phonemize is working!\")\n",
                "except ImportError as e:\n",
                "    print(f\"\\n‚ö†Ô∏è Import check failed: {e}. Trying to find egg manually...\")\n",
                "    # Standard fallback\n",
                "    # In SageMaker/Conda, it might install to local site-packages\n",
                "    pass"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Piper Setup & Data Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Install Python Dependencies (Torch 2.5.1)\n",
                "# Using specific versions to avoid PyTorch 2.6 'weights_only' issues\n",
                "!pip install -q torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1\n",
                "!pip install -q lightning==2.4.0\n",
                "!pip install -q librosa<1 numba==0.62.1\n",
                "!pip install -q onnx onnxruntime tensorboard tensorboardX\n",
                "!pip install -q pysilero-vad>=2.1 pathvalidate>=3\n",
                "!pip install -q phonemizer Unidecode tqdm inflect matplotlib pandas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "13493d59",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Clone Piper Repository & Patch Code\n",
                "import os\n",
                "\n",
                "# CLONE REPO\n",
                "if not os.path.exists(\"piper_repo\"):\n",
                "    !git clone https://github.com/rhasspy/piper.git piper_repo\n",
                "    print(\"‚úÖ Cloned piper repository\")\n",
                "else:\n",
                "    print(\"‚úÖ piper_repo already exists\")\n",
                "\n",
                "# --- PATCHING ---\n",
                "print(\"\\nü©π Applying PyTorch Lightning 2.x Patches...\")\n",
                "\n",
                "# 1. Fix __main__.py (Trainer compatibility)\n",
                "main_py_path = \"piper_repo/src/python/piper_train/__main__.py\"\n",
                "main_py_content = \"\"\"import argparse\n",
                "import json\n",
                "import logging\n",
                "from pathlib import Path\n",
                "\n",
                "import torch\n",
                "from pytorch_lightning import Trainer\n",
                "from pytorch_lightning.callbacks import ModelCheckpoint\n",
                "\n",
                "from .vits.lightning import VitsModel\n",
                "\n",
                "_LOGGER = logging.getLogger(__package__)\n",
                "\n",
                "\n",
                "def main():\n",
                "    logging.basicConfig(level=logging.DEBUG)\n",
                "\n",
                "    parser = argparse.ArgumentParser()\n",
                "    parser.add_argument(\n",
                "        \"--dataset-dir\", required=True, help=\"Path to pre-processed dataset directory\"\n",
                "    )\n",
                "    parser.add_argument(\n",
                "        \"--checkpoint-epochs\",\n",
                "        type=int,\n",
                "        help=\"Save checkpoint every N epochs (default: 1)\",\n",
                "    )\n",
                "    parser.add_argument(\n",
                "        \"--quality\",\n",
                "        default=\"medium\",\n",
                "        choices=(\"x-low\", \"medium\", \"high\"),\n",
                "        help=\"Quality/size of model (default: medium)\",\n",
                "    )\n",
                "    parser.add_argument(\n",
                "        \"--resume_from_single_speaker_checkpoint\",\n",
                "        help=\"For multi-speaker models only. Converts a single-speaker checkpoint to multi-speaker and resumes training\",\n",
                "    )\n",
                "    \n",
                "    # Manually add PL 2.x arguments that we use\n",
                "    parser.add_argument(\"--max_epochs\", type=int, default=1000)\n",
                "    parser.add_argument(\"--accelerator\", default=\"auto\")\n",
                "    parser.add_argument(\"--devices\", default=\"auto\")\n",
                "    parser.add_argument(\"--precision\", default=\"32-true\")\n",
                "    parser.add_argument(\"--default_root_dir\", type=str, default=None)\n",
                "    parser.add_argument(\"--resume_from_checkpoint\", type=str, default=None)\n",
                "\n",
                "    # Trainer.add_argparse_args(parser) # Removed in PL 2.0\n",
                "    VitsModel.add_model_specific_args(parser)\n",
                "    parser.add_argument(\"--seed\", type=int, default=1234)\n",
                "    args = parser.parse_args()\n",
                "    _LOGGER.debug(args)\n",
                "\n",
                "    args.dataset_dir = Path(args.dataset_dir)\n",
                "    if not args.default_root_dir:\n",
                "        args.default_root_dir = str(args.dataset_dir) # Must be string for Trainer explicitly\n",
                "    \n",
                "    torch.backends.cudnn.benchmark = True\n",
                "    torch.manual_seed(args.seed)\n",
                "\n",
                "    config_path = args.dataset_dir / \"config.json\"\n",
                "    dataset_path = args.dataset_dir / \"dataset.jsonl\"\n",
                "\n",
                "    with open(config_path, \"r\", encoding=\"utf-8\") as config_file:\n",
                "        config = json.load(config_file)\n",
                "        num_symbols = int(config[\"num_symbols\"])\n",
                "        num_speakers = int(config[\"num_speakers\"])\n",
                "        sample_rate = int(config[\"audio\"][\"sample_rate\"])\n",
                "\n",
                "\n",
                "    callbacks = []\n",
                "    if args.checkpoint_epochs is not None:\n",
                "        callbacks = [ModelCheckpoint(every_n_epochs=args.checkpoint_epochs)]\n",
                "        _LOGGER.debug(\n",
                "            \"Checkpoints will be saved every %s epoch(s)\", args.checkpoint_epochs\n",
                "        )\n",
                "\n",
                "    # Instantiate Trainer explicitly\n",
                "    trainer = Trainer(\n",
                "        max_epochs=args.max_epochs,\n",
                "        accelerator=args.accelerator,\n",
                "        devices=int(args.devices) if isinstance(args.devices, str) and args.devices.isdigit() else args.devices,\n",
                "        precision=args.precision,\n",
                "        default_root_dir=args.default_root_dir,\n",
                "        callbacks=callbacks\n",
                "    )\n",
                "\n",
                "    dict_args = vars(args)\n",
                "    if args.quality == \"x-low\":\n",
                "        dict_args[\"hidden_channels\"] = 96\n",
                "        dict_args[\"inter_channels\"] = 96\n",
                "        dict_args[\"filter_channels\"] = 384\n",
                "    elif args.quality == \"high\":\n",
                "        dict_args[\"resblock\"] = \"1\"\n",
                "        dict_args[\"resblock_kernel_sizes\"] = (3, 7, 11)\n",
                "        dict_args[\"resblock_dilation_sizes\"] = (\n",
                "            (1, 3, 5),\n",
                "            (1, 3, 5),\n",
                "            (1, 3, 5),\n",
                "        )\n",
                "        dict_args[\"upsample_rates\"] = (8, 8, 2, 2)\n",
                "        dict_args[\"upsample_initial_channel\"] = 512\n",
                "        dict_args[\"upsample_kernel_sizes\"] = (16, 16, 4, 4)\n",
                "\n",
                "    model = VitsModel(\n",
                "        num_symbols=num_symbols,\n",
                "        num_speakers=num_speakers,\n",
                "        sample_rate=sample_rate,\n",
                "        dataset=[dataset_path],\n",
                "        **dict_args,\n",
                "    )\n",
                "\n",
                "    if args.resume_from_single_speaker_checkpoint:\n",
                "        assert (\n",
                "            num_speakers > 1\n",
                "        ), \"--resume_from_single_speaker_checkpoint is only for multi-speaker models. Use --resume_from_checkpoint for single-speaker models.\"\n",
                "\n",
                "        # Load single-speaker checkpoint\n",
                "        _LOGGER.debug(\n",
                "            \"Resuming from single-speaker checkpoint: %s\",\n",
                "            args.resume_from_single_speaker_checkpoint,\n",
                "        )\n",
                "        model_single = VitsModel.load_from_checkpoint(\n",
                "            args.resume_from_single_speaker_checkpoint,\n",
                "            dataset=None,\n",
                "        )\n",
                "        g_dict = model_single.model_g.state_dict()\n",
                "        for key in list(g_dict.keys()):\n",
                "            # Remove keys that can't be copied over due to missing speaker embedding\n",
                "            if (\n",
                "                key.startswith(\"dec.cond\")\n",
                "                or key.startswith(\"dp.cond\")\n",
                "                or (\"enc.cond_layer\" in key)\n",
                "            ):\n",
                "                g_dict.pop(key, None)\n",
                "\n",
                "        # Copy over the multi-speaker model, excluding keys related to the\n",
                "        # speaker embedding (which is missing from the single-speaker model).\n",
                "        load_state_dict(model.model_g, g_dict)\n",
                "        load_state_dict(model.model_d, model_single.model_d.state_dict())\n",
                "        _LOGGER.info(\n",
                "            \"Successfully converted single-speaker checkpoint to multi-speaker\"\n",
                "        )\n",
                "\n",
                "    ckpt_path = args.resume_from_checkpoint\n",
                "    if args.resume_from_single_speaker_checkpoint:\n",
                "        ckpt_path = None # We manually loaded weights, start fresh\n",
                "\n",
                "    trainer.fit(model, ckpt_path=ckpt_path)\n",
                "\n",
                "\n",
                "def load_state_dict(model, saved_state_dict):\n",
                "    state_dict = model.state_dict()\n",
                "    new_state_dict = {}\n",
                "\n",
                "    for k, v in state_dict.items():\n",
                "        if k in saved_state_dict:\n",
                "            # Use saved value\n",
                "            new_state_dict[k] = saved_state_dict[k]\n",
                "        else:\n",
                "            # Use initialized value\n",
                "            _LOGGER.debug(\"%s is not in the checkpoint\", k)\n",
                "            new_state_dict[k] = v\n",
                "\n",
                "    model.load_state_dict(new_state_dict)\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()\n",
                "\"\"\"\n",
                "with open(main_py_path, \"w\") as f:\n",
                "    f.write(main_py_content)\n",
                "print(\"‚úÖ Fixed piper_train/__main__.py\")\n",
                "\n",
                "# 2. Fix lightning.py (Manual Optimization)\n",
                "lightning_py_path = \"piper_repo/src/python/piper_train/vits/lightning.py\"\n",
                "lightning_py_content = \"\"\"import logging\n",
                "from pathlib import Path\n",
                "from typing import List, Optional, Tuple, Union\n",
                "\n",
                "import pytorch_lightning as pl\n",
                "import torch\n",
                "from torch import autocast\n",
                "from torch.nn import functional as F\n",
                "from torch.utils.data import DataLoader, Dataset, random_split\n",
                "\n",
                "from .commons import slice_segments\n",
                "from .dataset import Batch, PiperDataset, UtteranceCollate\n",
                "from .losses import discriminator_loss, feature_loss, generator_loss, kl_loss\n",
                "from .mel_processing import mel_spectrogram_torch, spec_to_mel_torch\n",
                "from .models import MultiPeriodDiscriminator, SynthesizerTrn\n",
                "\n",
                "_LOGGER = logging.getLogger(\"vits.lightning\")\n",
                "\n",
                "\n",
                "class VitsModel(pl.LightningModule):\n",
                "    def __init__(\n",
                "        self,\n",
                "        num_symbols: int,\n",
                "        num_speakers: int,\n",
                "        # audio\n",
                "        resblock=\"2\",\n",
                "        resblock_kernel_sizes=(3, 5, 7),\n",
                "        resblock_dilation_sizes=(\n",
                "            (1, 2),\n",
                "            (2, 6),\n",
                "            (3, 12),\n",
                "        ),\n",
                "        upsample_rates=(8, 8, 4),\n",
                "        upsample_initial_channel=256,\n",
                "        upsample_kernel_sizes=(16, 16, 8),\n",
                "        # mel\n",
                "        filter_length: int = 1024,\n",
                "        hop_length: int = 256,\n",
                "        win_length: int = 1024,\n",
                "        mel_channels: int = 80,\n",
                "        sample_rate: int = 22050,\n",
                "        sample_bytes: int = 2,\n",
                "        channels: int = 1,\n",
                "        mel_fmin: float = 0.0,\n",
                "        mel_fmax: Optional[float] = None,\n",
                "        # model\n",
                "        inter_channels: int = 192,\n",
                "        hidden_channels: int = 192,\n",
                "        filter_channels: int = 768,\n",
                "        n_heads: int = 2,\n",
                "        n_layers: int = 6,\n",
                "        kernel_size: int = 3,\n",
                "        p_dropout: float = 0.1,\n",
                "        n_layers_q: int = 3,\n",
                "        use_spectral_norm: bool = False,\n",
                "        gin_channels: int = 0,\n",
                "        use_sdp: bool = True,\n",
                "        segment_size: int = 8192,\n",
                "        # training\n",
                "        dataset: Optional[List[Union[str, Path]]] = None,\n",
                "        learning_rate: float = 2e-4,\n",
                "        betas: Tuple[float, float] = (0.8, 0.99),\n",
                "        eps: float = 1e-9,\n",
                "        batch_size: int = 1,\n",
                "        lr_decay: float = 0.999875,\n",
                "        init_lr_ratio: float = 1.0,\n",
                "        warmup_epochs: int = 0,\n",
                "        c_mel: int = 45,\n",
                "        c_kl: float = 1.0,\n",
                "        grad_clip: Optional[float] = None,\n",
                "        num_workers: int = 1,\n",
                "        seed: int = 1234,\n",
                "        num_test_examples: int = 5,\n",
                "        validation_split: float = 0.1,\n",
                "        max_phoneme_ids: Optional[int] = None,\n",
                "        **kwargs,\n",
                "    ):\n",
                "        super().__init__()\n",
                "        self.save_hyperparameters()\n",
                "        \n",
                "        # Lightning 2.x requires manual optimization for multiple optimizers\n",
                "        self.automatic_optimization = False\n",
                "\n",
                "        if (self.hparams.num_speakers > 1) and (self.hparams.gin_channels <= 0):\n",
                "            # Default gin_channels for multi-speaker model\n",
                "            self.hparams.gin_channels = 512\n",
                "\n",
                "        # Set up models\n",
                "        self.model_g = SynthesizerTrn(\n",
                "            n_vocab=self.hparams.num_symbols,\n",
                "            spec_channels=self.hparams.filter_length // 2 + 1,\n",
                "            segment_size=self.hparams.segment_size // self.hparams.hop_length,\n",
                "            inter_channels=self.hparams.inter_channels,\n",
                "            hidden_channels=self.hparams.hidden_channels,\n",
                "            filter_channels=self.hparams.filter_channels,\n",
                "            n_heads=self.hparams.n_heads,\n",
                "            n_layers=self.hparams.n_layers,\n",
                "            kernel_size=self.hparams.kernel_size,\n",
                "            p_dropout=self.hparams.p_dropout,\n",
                "            resblock=self.hparams.resblock,\n",
                "            resblock_kernel_sizes=self.hparams.resblock_kernel_sizes,\n",
                "            resblock_dilation_sizes=self.hparams.resblock_dilation_sizes,\n",
                "            upsample_rates=self.hparams.upsample_rates,\n",
                "            upsample_initial_channel=self.hparams.upsample_initial_channel,\n",
                "            upsample_kernel_sizes=self.hparams.upsample_kernel_sizes,\n",
                "            n_speakers=self.hparams.num_speakers,\n",
                "            gin_channels=self.hparams.gin_channels,\n",
                "            use_sdp=self.hparams.use_sdp,\n",
                "        )\n",
                "        self.model_d = MultiPeriodDiscriminator(\n",
                "            use_spectral_norm=self.hparams.use_spectral_norm\n",
                "        )\n",
                "\n",
                "        # Dataset splits\n",
                "        self._train_dataset: Optional[Dataset] = None\n",
                "        self._val_dataset: Optional[Dataset] = None\n",
                "        self._test_dataset: Optional[Dataset] = None\n",
                "        self._load_datasets(validation_split, num_test_examples, max_phoneme_ids)\n",
                "\n",
                "        # State kept between training optimizers\n",
                "        self._y = None\n",
                "        self._y_hat = None\n",
                "\n",
                "    def _load_datasets(\n",
                "        self,\n",
                "        validation_split: float,\n",
                "        num_test_examples: int,\n",
                "        max_phoneme_ids: Optional[int] = None,\n",
                "    ):\n",
                "        if self.hparams.dataset is None:\n",
                "            _LOGGER.debug(\"No dataset to load\")\n",
                "            return\n",
                "\n",
                "        full_dataset = PiperDataset(\n",
                "            self.hparams.dataset, max_phoneme_ids=max_phoneme_ids\n",
                "        )\n",
                "        valid_set_size = int(len(full_dataset) * validation_split)\n",
                "        train_set_size = len(full_dataset) - valid_set_size - num_test_examples\n",
                "\n",
                "        self._train_dataset, self._test_dataset, self._val_dataset = random_split(\n",
                "            full_dataset, [train_set_size, num_test_examples, valid_set_size]\n",
                "        )\n",
                "\n",
                "    def forward(self, text, text_lengths, scales, sid=None):\n",
                "        noise_scale = scales[0]\n",
                "        length_scale = scales[1]\n",
                "        noise_scale_w = scales[2]\n",
                "        audio, *_ = self.model_g.infer(\n",
                "            text,\n",
                "            text_lengths,\n",
                "            noise_scale=noise_scale,\n",
                "            length_scale=length_scale,\n",
                "            noise_scale_w=noise_scale_w,\n",
                "            sid=sid,\n",
                "        )\n",
                "\n",
                "        return audio\n",
                "\n",
                "    def train_dataloader(self):\n",
                "        return DataLoader(\n",
                "            self._train_dataset,\n",
                "            collate_fn=UtteranceCollate(\n",
                "                is_multispeaker=self.hparams.num_speakers > 1,\n",
                "                segment_size=self.hparams.segment_size,\n",
                "            ),\n",
                "            num_workers=self.hparams.num_workers,\n",
                "            batch_size=self.hparams.batch_size,\n",
                "        )\n",
                "\n",
                "    def val_dataloader(self):\n",
                "        return DataLoader(\n",
                "            self._val_dataset,\n",
                "            collate_fn=UtteranceCollate(\n",
                "                is_multispeaker=self.hparams.num_speakers > 1,\n",
                "                segment_size=self.hparams.segment_size,\n",
                "            ),\n",
                "            num_workers=self.hparams.num_workers,\n",
                "            batch_size=self.hparams.batch_size,\n",
                "        )\n",
                "\n",
                "    def test_dataloader(self):\n",
                "        return DataLoader(\n",
                "            self._test_dataset,\n",
                "            collate_fn=UtteranceCollate(\n",
                "                is_multispeaker=self.hparams.num_speakers > 1,\n",
                "                segment_size=self.hparams.segment_size,\n",
                "            ),\n",
                "            num_workers=self.hparams.num_workers,\n",
                "            batch_size=self.hparams.batch_size,\n",
                "        )\n",
                "\n",
                "    def training_step(self, batch: Batch, batch_idx: int):\n",
                "        # Manual optimization for Lightning 2.x with multiple optimizers\n",
                "        opt_g, opt_d = self.optimizers()\n",
                "        \n",
                "        # Train Generator\n",
                "        loss_gen_all = self.training_step_g(batch)\n",
                "        opt_g.zero_grad()\n",
                "        self.manual_backward(loss_gen_all)\n",
                "        opt_g.step()\n",
                "        \n",
                "        # Train Discriminator\n",
                "        loss_disc_all = self.training_step_d(batch)\n",
                "        opt_d.zero_grad()\n",
                "        self.manual_backward(loss_disc_all)\n",
                "        opt_d.step()\n",
                "        \n",
                "        # Step learning rate schedulers\n",
                "        sch_g, sch_d = self.lr_schedulers()\n",
                "        if self.trainer.is_last_batch:\n",
                "             sch_g.step()\n",
                "             sch_d.step()\n",
                "\n",
                "\n",
                "    def training_step_g(self, batch: Batch):\n",
                "        x, x_lengths, y, _, spec, spec_lengths, speaker_ids = (\n",
                "            batch.phoneme_ids,\n",
                "            batch.phoneme_lengths,\n",
                "            batch.audios,\n",
                "            batch.audio_lengths,\n",
                "            batch.spectrograms,\n",
                "            batch.spectrogram_lengths,\n",
                "            batch.speaker_ids if batch.speaker_ids is not None else None,\n",
                "        )\n",
                "        (\n",
                "            y_hat,\n",
                "            l_length,\n",
                "            _attn,\n",
                "            ids_slice,\n",
                "            _x_mask,\n",
                "            z_mask,\n",
                "            (_z, z_p, m_p, logs_p, _m_q, logs_q),\n",
                "        ) = self.model_g(x, x_lengths, spec, spec_lengths, speaker_ids)\n",
                "        self._y_hat = y_hat\n",
                "\n",
                "        mel = spec_to_mel_torch(\n",
                "            spec,\n",
                "            self.hparams.filter_length,\n",
                "            self.hparams.mel_channels,\n",
                "            self.hparams.sample_rate,\n",
                "            self.hparams.mel_fmin,\n",
                "            self.hparams.mel_fmax,\n",
                "        )\n",
                "        y_mel = slice_segments(\n",
                "            mel,\n",
                "            ids_slice,\n",
                "            self.hparams.segment_size // self.hparams.hop_length,\n",
                "        )\n",
                "        y_hat_mel = mel_spectrogram_torch(\n",
                "            y_hat.squeeze(1),\n",
                "            self.hparams.filter_length,\n",
                "            self.hparams.mel_channels,\n",
                "            self.hparams.sample_rate,\n",
                "            self.hparams.hop_length,\n",
                "            self.hparams.win_length,\n",
                "            self.hparams.mel_fmin,\n",
                "            self.hparams.mel_fmax,\n",
                "        )\n",
                "        y = slice_segments(\n",
                "            y,\n",
                "            ids_slice * self.hparams.hop_length,\n",
                "            self.hparams.segment_size,\n",
                "        )  # slice\n",
                "\n",
                "        # Save for training_step_d\n",
                "        self._y = y\n",
                "\n",
                "        _y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = self.model_d(y, y_hat)\n",
                "\n",
                "        with autocast(self.device.type, enabled=False):\n",
                "            # Generator loss\n",
                "            loss_dur = torch.sum(l_length.float())\n",
                "            loss_mel = F.l1_loss(y_mel, y_hat_mel) * self.hparams.c_mel\n",
                "            loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * self.hparams.c_kl\n",
                "\n",
                "            loss_fm = feature_loss(fmap_r, fmap_g)\n",
                "            loss_gen, _losses_gen = generator_loss(y_d_hat_g)\n",
                "            loss_gen_all = loss_gen + loss_fm + loss_mel + loss_dur + loss_kl\n",
                "\n",
                "            self.log(\"loss_gen_all\", loss_gen_all, prog_bar=True)\n",
                "\n",
                "            return loss_gen_all\n",
                "\n",
                "    def training_step_d(self, batch: Batch):\n",
                "        # From training_step_g\n",
                "        y = self._y\n",
                "        y_hat = self._y_hat\n",
                "        y_d_hat_r, y_d_hat_g, _, _ = self.model_d(y, y_hat.detach())\n",
                "\n",
                "        with autocast(self.device.type, enabled=False):\n",
                "            # Discriminator\n",
                "            loss_disc, _losses_disc_r, _losses_disc_g = discriminator_loss(\n",
                "                y_d_hat_r, y_d_hat_g\n",
                "            )\n",
                "            loss_disc_all = loss_disc\n",
                "\n",
                "            self.log(\"loss_disc_all\", loss_disc_all, prog_bar=True)\n",
                "\n",
                "            return loss_disc_all\n",
                "\n",
                "    def validation_step(self, batch: Batch, batch_idx: int):\n",
                "        val_loss = self.training_step_g(batch) + self.training_step_d(batch)\n",
                "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
                "\n",
                "        # Generate audio examples\n",
                "        for utt_idx, test_utt in enumerate(self._test_dataset):\n",
                "            text = test_utt.phoneme_ids.unsqueeze(0).to(self.device)\n",
                "            text_lengths = torch.LongTensor([len(test_utt.phoneme_ids)]).to(self.device)\n",
                "            scales = [0.667, 1.0, 0.8]\n",
                "            sid = (\n",
                "                test_utt.speaker_id.to(self.device)\n",
                "                if test_utt.speaker_id is not None\n",
                "                else None\n",
                "            )\n",
                "            test_audio = self(text, text_lengths, scales, sid=sid).detach()\n",
                "\n",
                "            # Scale to make louder in [-1, 1]\n",
                "            test_audio = test_audio * (1.0 / max(0.01, abs(test_audio.max())))\n",
                "\n",
                "            tag = test_utt.text or str(utt_idx)\n",
                "            self.logger.experiment.add_audio(\n",
                "                tag, test_audio, sample_rate=self.hparams.sample_rate\n",
                "            )\n",
                "\n",
                "        return val_loss\n",
                "\n",
                "    def configure_optimizers(self):\n",
                "        optimizers = [\n",
                "            torch.optim.AdamW(\n",
                "                self.model_g.parameters(),\n",
                "                lr=self.hparams.learning_rate,\n",
                "                betas=self.hparams.betas,\n",
                "                eps=self.hparams.eps,\n",
                "            ),\n",
                "            torch.optim.AdamW(\n",
                "                self.model_d.parameters(),\n",
                "                lr=self.hparams.learning_rate,\n",
                "                betas=self.hparams.betas,\n",
                "                eps=self.hparams.eps,\n",
                "            ),\n",
                "        ]\n",
                "        schedulers = [\n",
                "            torch.optim.lr_scheduler.ExponentialLR(\n",
                "                optimizers[0], gamma=self.hparams.lr_decay\n",
                "            ),\n",
                "            torch.optim.lr_scheduler.ExponentialLR(\n",
                "                optimizers[1], gamma=self.hparams.lr_decay\n",
                "            ),\n",
                "        ]\n",
                "\n",
                "        return [\n",
                "            {\"optimizer\": optimizers[0], \"lr_scheduler\": schedulers[0]},\n",
                "            {\"optimizer\": optimizers[1], \"lr_scheduler\": schedulers[1]},\n",
                "        ]\n",
                "\n",
                "    @staticmethod\n",
                "    def add_model_specific_args(parent_parser):\n",
                "        parser = parent_parser.add_argument_group(\"VitsModel\")\n",
                "        parser.add_argument(\"--batch-size\", type=int, required=True)\n",
                "        parser.add_argument(\"--validation-split\", type=float, default=0.1)\n",
                "        parser.add_argument(\"--num-test-examples\", type=int, default=5)\n",
                "        parser.add_argument(\n",
                "            \"--max-phoneme-ids\",\n",
                "            type=int,\n",
                "            help=\"Exclude utterances with phoneme id lists longer than this\",\n",
                "        )\n",
                "        #\n",
                "        parser.add_argument(\"--hidden-channels\", type=int, default=192)\n",
                "        parser.add_argument(\"--inter-channels\", type=int, default=192)\n",
                "        parser.add_argument(\"--filter-channels\", type=int, default=768)\n",
                "        parser.add_argument(\"--n-layers\", type=int, default=6)\n",
                "        parser.add_argument(\"--n-heads\", type=int, default=2)\n",
                "        #\n",
                "        return parent_parser\n",
                "\"\"\"\n",
                "with open(lightning_py_path, \"w\") as f:\n",
                "    f.write(lightning_py_content)\n",
                "print(\"‚úÖ Fixed piper_train/vits/lightning.py\")\n",
                "\n",
                "# 3. Fix monotonic_align/__init__.py\n",
                "monotonic_init = \"piper_repo/src/python/piper_train/vits/monotonic_align/__init__.py\"\n",
                "if os.path.exists(monotonic_init):\n",
                "    with open(monotonic_init, \"r\") as f:\n",
                "        content = f.read()\n",
                "    content = content.replace(\"from .monotonic_align.core\", \"from .core\")\n",
                "    with open(monotonic_init, \"w\") as f:\n",
                "        f.write(content)\n",
                "    print(\"‚úÖ Fixed monotonic_align import\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Build Monotonic Align (Required for VITS)\n",
                "import os\n",
                "import shutil\n",
                "\n",
                "piper_src_path = os.path.abspath(\"piper_repo/src/python\")\n",
                "monotonic_align_src = os.path.join(piper_src_path, \"piper_train/vits/monotonic_align\")\n",
                "\n",
                "# Build directly in place since we have full permissions usually\n",
                "# But to be safe and clean, use temp\n",
                "temp_build_dir = \"monotonic_align_build\"\n",
                "if os.path.exists(temp_build_dir):\n",
                "    shutil.rmtree(temp_build_dir)\n",
                "os.makedirs(temp_build_dir, exist_ok=True)\n",
                "\n",
                "for filename in [\"core.pyx\", \"setup.py\"]:\n",
                "    shutil.copy(os.path.join(monotonic_align_src, filename), os.path.join(temp_build_dir, filename))\n",
                "\n",
                "%cd {temp_build_dir}\n",
                "print(\"üî® Building monotonic_align...\")\n",
                "!python setup.py build_ext --inplace\n",
                "\n",
                "import glob\n",
                "so_files = glob.glob(\"core*.so\")\n",
                "if so_files:\n",
                "    dest = os.path.join(monotonic_align_src, so_files[0])\n",
                "    shutil.copy(so_files[0], dest)\n",
                "    print(f\"‚úÖ Installed compiled extension to: {dest}\")\n",
                "else:\n",
                "    print(\"‚ùå Build failed, no .so file found\")\n",
                "\n",
                "%cd .."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Run Preprocessing\n",
                "import os\n",
                "import sys\n",
                "\n",
                "piper_src_path = os.path.abspath(\"piper_repo/src/python\")\n",
                "DATASET_PATH = os.path.abspath(\"english\") # Ensure this matches your uploaded folder name\n",
                "\n",
                "print(f\"üìÇ Dataset: {DATASET_PATH}\")\n",
                "\n",
                "# Add piper_phonemize to path if needed (though 'setup.py install' should have handled it)\n",
                "# We strictly set PYTHONPATH to include piper src\n",
                "\n",
                "!PYTHONPATH=\"{piper_src_path}\" python -m piper_train.preprocess \\\n",
                "  --language en \\\n",
                "  --input-dir \"{DATASET_PATH}\" \\\n",
                "  --output-dir training_dir \\\n",
                "  --dataset-format ljspeech \\\n",
                "  --single-speaker \\\n",
                "  --sample-rate 22050\n",
                "\n",
                "print(\"\\n‚úÖ Preprocessing complete (if no errors above)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Download Base Checkpoint\n",
                "import urllib.request\n",
                "\n",
                "os.makedirs(\"checkpoints\", exist_ok=True)\n",
                "checkpoint_url = \"https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/en/en_US/lessac/medium/epoch%3D2164-step%3D1355540.ckpt\"\n",
                "checkpoint_path = \"checkpoints/epoch=2164-step=1355540.ckpt\"\n",
                "\n",
                "if not os.path.exists(checkpoint_path):\n",
                "    print(\"üì• Downloading base checkpoint...\")\n",
                "    urllib.request.urlretrieve(checkpoint_url, checkpoint_path)\n",
                "    print(\"‚úÖ Downloaded!\")\n",
                "else:\n",
                "    print(\"‚úÖ Checkpoint already exists\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Start Training\n",
                "\n",
                "# Determine Accelerator (SageMaker might have different GPU count)\n",
                "import torch\n",
                "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
                "devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
                "print(f\"üöÄ Training on {devices} {accelerator}(s)\")\n",
                "\n",
                "# Note: Batch size 16 is safe for T4 (g4dn.xlarge). If using A10g (g5.xlarge), you can try 32 or 64.\n",
                "!PYTHONPATH=\"{piper_src_path}\" python -m piper_train \\\n",
                "  --dataset-dir training_dir \\\n",
                "  --accelerator {accelerator} \\\n",
                "  --devices {devices} \\\n",
                "  --batch-size 16 \\\n",
                "  --validation-split 0.0 \\\n",
                "  --num-test-examples 0 \\\n",
                "  --max_epochs 10000 \\\n",
                "  --resume_from_checkpoint \"{checkpoint_path}\" \\\n",
                "  --checkpoint-epochs 1 \\\n",
                "  --quality medium \\\n",
                "  --precision 32"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "conda_pytorch_p310",
            "language": "python",
            "name": "conda_pytorch_p310"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

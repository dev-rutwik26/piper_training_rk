{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc01a55",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UPN9ss5mOaY0"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c68027",
   "metadata": {},
   "source": [
    "### Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found zip file at: ../hindi_female_english.zip\n",
      "Successfully extracted to 'dataset/'\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "# Unzip the main dataset file from the parent directory\n",
    "zip_path = '../hindi_female_english.zip'\n",
    "output_dir = 'dataset'\n",
    "if os.path.exists(zip_path):\n",
    "    print(f\"Found zip file at: {zip_path}\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "        print(f\"Successfully extracted to '{output_dir}/'\")\n",
    "else:\n",
    "    print(f\"Error: File not found at {zip_path}\")\n",
    "    print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa159e4",
   "metadata": {},
   "source": [
    "### Some Analysis of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "bKwYT_ZgYYcV"
   },
   "outputs": [],
   "source": [
    "path_to_file = 'dataset/english/txt.done.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "UFu7680EYYkd"
   },
   "outputs": [],
   "source": [
    "text = open(path_to_file, 'r',encoding='utf-8',\n",
    "                 errors='ignore').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwiIy8uAg5uW",
    "outputId": "08a878ad-b45c-431d-992f-2b9a7515533c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( train_hindifullfemale_00001 \" Author of the danger trail, Philip Steels, etc. \" )\n",
      "( train_hindifullfemale_00002 \" Not at this particular case, Tom, apologized Whittemore. \" )\n",
      "( train_hindifullfemale_00003 \" For the twentieth time that evening the two men shook hands. \" )\n",
      "( train_hindifullfemale_00004 \" Lord, but I'm glad to see you again, Phil. \" )\n",
      "( train_hindifullfemale_00005 \" Will we ever forget it. \" )\n",
      "( train_hindifullfemale_00006 \" God bless 'em, I hope I'll go on seeing them forever. \" )\n",
      "( train_hindifullfemale_00007 \" And you always want to see it in the superlative degree. \" )\n",
      "( train_hindifullfemale_00008 \" Gad, your letter came just in time. \" )\n",
      "( train_hindifullfemale_00009 \" He turned sharply, and faced Gregson across the table. \" )\n",
      "( train_hindifullfemale_00010 \" I'm playing a single hand in what looks like a losing game. \" )\n",
      "( train_hindifullfemale_00011 \" If I ever needed a fighter in my life I need one now. \" )\n",
      "( train_hindifullfemale_00012 \" Gregson shoved back his chair and rose to his feet. \" )\n",
      "( train_hindifullfemale_00013 \" He was a head shorter than his companion, of almost delicate physique. \" )\n",
      "( train_hindifullfemale_00014 \" Now you're coming down to business, Phil, he exclaimed. \" )\n",
      "( train_hindifullfemale_00015 \" It is the aurora borealis. \" )\n",
      "( train_hindifullfemale_00016 \" There's Fort Churchill, a rifle shot beyond the ridge, asleep. \" )\n",
      "( train_hindifullfemale_00017 \" From that moment his friendship for Belize turns to hatred and jealousy. \" )\n",
      "( train_hindifullfemale_00018 \" There was a change now. \" )\n",
      "( train_hindifullfemale_00019 \" I followed the line of the proposed railroad, looking for chances. \" )\n",
      "( train_hindifullfemale_00020 \" Clubs and balls and cities grew to be only memories. \" )\n",
      "( train_hindifullfemale_00021 \" It fairly clubbed me into recognizing it. \" )\n",
      "( train_hindifullfemale_00022 \" Hardly were our plans made public before we were met by powerful opposition. \" )\n",
      "( train_hindifullfemale_00023 \" A combination of Canadian capital quickly organized and petitioned for the same privileges. \" )\n",
      "( train_hindifullfemale_00024 \" It was my reports from the north which chiefly induced people to buy. \" )\n",
      "( train_hindifullfemale_00025 \" I was about to do this when cooler judgment prevailed. \" )\n",
      "( train_hindifullfemale_00026 \" It occurred to me that there would have to be an accounting. \" )\n",
      "( train_hindifullfemale_00027 \" To my surprise he began to show actual enthusiasm in my favor. \" )\n",
      "( train_hindifullfemale_00028 \" Robbery, bribery, fraud, \" )\n",
      "( train_hindifullfemale_00029 \" Their forces were already moving into the north country. \" )\n",
      "( train_hindifullfemale_00030 \" I had faith in them. \" )\n",
      "( train_hindifullfemale_00031 \" They were three hundred yards apart. \" )\n",
      "( train_hindifullfemale_00032 \" Since then some mysterious force has been fighting us at every step. \" )\n",
      "( train_hindifullfemale_00033 \" He unfolded a long typewritten letter, and handed it to Gregson. \" )\n",
      "( train_hindifullfemale_00034 \" Men of Selden's stamp don't stop at women and children. \" )\n",
      "( train_hindifullfemale_00035 \" He stopped, and Philip nodded at the horrified question in his eyes. \" )\n",
      "( train_hindifullfemale_00036 \" She turned in at the hotel. \" )\n",
      "( train_hindifullfemale_00037 \" I was the only one who remained sitting. \" )\n",
      "( train_hindifullfemale_00038 \" We'll have to watch our chances. \" )\n",
      "( train_hindifullfemale_00039 \" The ship should be in within a week or ten days. \" )\n",
      "( train_hindifullfemale_00040 \" I suppose you wonder why she is coming up here. \" )\n",
      "( train_hindifullfemale_00041 \" Meanwhile I'll go out to breathe a spell. \" )\n",
      "( train_hindifullfemale_00042 \" How could he explain his possession of the sketch. \" )\n",
      "( train_hindifullfemale_00043 \" It seemed nearer to him since he had seen and talked with Gregson. \" )\n",
      "( train_hindifullfemale_00044 \" Her own betrayal of herself was like tonic to Philip. \" )\n",
      "( train_hindifullfemale_00045 \" He moved away as quietly as he had come. \" )\n",
      "( train_hindifullfemale_00046 \" The girl faced him, her eyes shining with sudden fear. \" )\n",
      "( train_hindifullfemale_00047 \" Close beside him gleamed the white fangs of the wolf dog. \" )\n",
      "( train_hindifullfemale_00048 \" He looked at the handkerchief more, closely. \" )\n",
      "( train_hindifullfemale_00049 \" Gregson was asleep when he re entered the cabin. \" )\n",
      "( train_hindifullfemale_00050 \" In spite of their absurdity the words affected Philip curiously. \" )\n",
      "( train_hindifullfemale_00051 \" The lace was of a delicate ivory color, faintly tinted with yellow. \" )\n",
      "( train_hindifullfemale_00052 \" It was a curious coincidence. \" )\n",
      "( train_hindifullfemale_00053 \" Suddenly his fingers closed tightly over the handkerchief. \" )\n",
      "( train_hindifullfemale_00054 \" There was nothing on the rock. \" )\n",
      "( train_hindifullfemale_00055 \" Philip stood undecided, his ears strained to catch the slightest sound. \" )\n",
      "( train_hindifullfemale_00056 \" Pearce's little eyes were fixed on him shrewdly. \" )\n",
      "( train_hindifullfemale_00057 \" I have no idea, replied\n"
     ]
    }
   ],
   "source": [
    "print(text[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qXwFnClVg9Tc",
    "outputId": "98761e2b-42cd-4a64-8143-49e3797beab1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '\"', \"'\", '(', ')', ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ';', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'W', 'Y', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(vocab)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0DqbY2zdhCEn",
    "outputId": "d00cd80a-bd91-4e46-bb88-e6379d9a6bbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind = {u:i for i, u in enumerate(vocab)}\n",
    "ind_to_char = np.array(vocab)\n",
    "encoded_text = np.array([char_to_ind[c] for c in text])\n",
    "seq_len = 250\n",
    "total_num_seq = len(text)//(seq_len+1)\n",
    "total_num_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "EE41QOwihHeS"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "\n",
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)\n",
    "\n",
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1]\n",
    "    target_txt = seq[1:]\n",
    "    return input_txt, target_txt\n",
    "\n",
    "dataset = sequences.map(create_seq_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a0a8c",
   "metadata": {},
   "source": [
    "### Forming the metadata.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "h5BgyakeNkRI",
    "outputId": "ad744268-d771-4188-c418-22824adac749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_hindifullfemale_00001</td>\n",
       "      <td>Author of the danger trail, Philip Steels, etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_hindifullfemale_00002</td>\n",
       "      <td>Not at this particular case, Tom, apologized W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_hindifullfemale_00003</td>\n",
       "      <td>For the twentieth time that evening the two me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_hindifullfemale_00004</td>\n",
       "      <td>Lord, but I'm glad to see you again, Phil.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_hindifullfemale_00005</td>\n",
       "      <td>Will we ever forget it.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0  \\\n",
       "0  train_hindifullfemale_00001   \n",
       "1  train_hindifullfemale_00002   \n",
       "2  train_hindifullfemale_00003   \n",
       "3  train_hindifullfemale_00004   \n",
       "4  train_hindifullfemale_00005   \n",
       "\n",
       "                                                   1  \n",
       "0    Author of the danger trail, Philip Steels, etc.  \n",
       "1  Not at this particular case, Tom, apologized W...  \n",
       "2  For the twentieth time that evening the two me...  \n",
       "3         Lord, but I'm glad to see you again, Phil.  \n",
       "4                            Will we ever forget it.  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and clean\n",
    "metadata_female_fp = 'dataset/english/txt.done.data'\n",
    "\n",
    "# Read file split by double quotes (to separate filename and transcript)\n",
    "metadata_female = pd.read_csv(metadata_female_fp, sep='\"', usecols=[0, 1], header=None)\n",
    "\n",
    "# Clean filename: remove leading '(' and whitespace\n",
    "metadata_female[0] = metadata_female[0].str.replace(r'\\(', '', regex=True).str.strip()\n",
    "\n",
    "# Clean transcript: remove leading/trailing whitespace\n",
    "metadata_female[1] = metadata_female[1].str.strip()\n",
    "\n",
    "# # Optional: add speaker info\n",
    "# metadata_female[2] = 'female'\n",
    "\n",
    "# Show shape and preview\n",
    "print(metadata_female.shape)\n",
    "metadata_female.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcce591",
   "metadata": {},
   "source": [
    "### Bulding the metadata.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "pfBupyWZORfp"
   },
   "outputs": [],
   "source": [
    "metadata = metadata_female\n",
    "metadata.to_csv('dataset/english/metadata_indic.csv', sep='|', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6bc45",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VfTlT8gFhRTd",
    "outputId": "63a9c695-0812-4d20-88cb-a53f8b1049d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Path: dataset/english/metadata_indic.csv\n",
      "Piper Source Path: /Users/rutwik/piper-model-training/piper_repo/src/python\n",
      "INFO:preprocess:Single speaker dataset\n",
      "INFO:preprocess:Wrote dataset config\n",
      "INFO:preprocess:Processing 21 utterance(s) with 12 worker(s)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 1. Define paths\n",
    "# Absolute path to your dataset folder (the one containing metadata.csv)\n",
    "dataset_path = 'dataset/english/metadata_indic.csv'\n",
    "\n",
    "# Absolute path to the 'src/python' folder inside the cloned piper_repo\n",
    "# This is where the 'piper_train' python package lives\n",
    "piper_src_path = os.path.abspath(\"../piper_repo/src/python\")\n",
    "\n",
    "print(f\"Dataset Path: {dataset_path}\")\n",
    "print(f\"Piper Source Path: {piper_src_path}\")\n",
    "\n",
    "# 2. Run the command with PYTHONPATH set\n",
    "# This tells Python: \"Look in 'piper_src_path' when I ask for 'piper_train'\"\n",
    "!PYTHONPATH=\"{piper_src_path}\" python3 -m piper_train.preprocess \\\n",
    "  --language en \\\n",
    "  --input-dir \"dataset/english\" \\\n",
    "  --output-dir training_dir_ruru \\\n",
    "  --dataset-format ljspeech \\\n",
    "  --single-speaker \\\n",
    "  --sample-rate 22050"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd29f8b",
   "metadata": {},
   "source": [
    "### Building the monotonic align "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created temp_build\n",
      "Copied core.pyx\n",
      "Copied setup.py\n",
      "/Users/rutwik/piper-model-training/scripts/temp_build\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 1. Define paths\n",
    "# Points to: /Users/rutwik/piper-model-training/piper_repo/src/python\n",
    "piper_src_path = os.path.abspath(\"../piper_repo/src/python\")\n",
    "\n",
    "# Source files we need to compile\n",
    "monotonic_align_src = os.path.join(piper_src_path, \"piper_train/vits/monotonic_align\")\n",
    "temp_build_dir = \"temp_build\"\n",
    "\n",
    "# 2. Create temp directory\n",
    "os.makedirs(temp_build_dir, exist_ok=True)\n",
    "print(f\"Created {temp_build_dir}\")\n",
    "\n",
    "# 3. Copy necessary files (core.pyx and setup.py)\n",
    "for filename in [\"core.pyx\", \"setup.py\"]:\n",
    "    src_file = os.path.join(monotonic_align_src, filename)\n",
    "    dst_file = os.path.join(temp_build_dir, filename)\n",
    "    if os.path.exists(src_file):\n",
    "        shutil.copy(src_file, dst_file)\n",
    "        print(f\"Copied {filename}\")\n",
    "    else:\n",
    "        print(f\"Error: Could not find {filename} at {src_file}\")\n",
    "\n",
    "# 4. Move into the build directory\n",
    "%cd {temp_build_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d08434b",
   "metadata": {},
   "source": [
    "### Copying the monotonic align file core.so "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guWq3XUgzeQ_",
    "outputId": "8bcd42b6-6e56-4c5e-9d8b-e98b869ee06f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling /Users/rutwik/piper-model-training/scripts/temp_build/core.pyx because it changed.\n",
      "[1/1] Cythonizing /Users/rutwik/piper-model-training/scripts/temp_build/core.pyx\n",
      "/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /Users/rutwik/piper-model-training/scripts/temp_build/core.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "ld: warning: duplicate -rpath '/Users/rutwik/miniconda3/envs/voice_training/lib' ignored\n",
      "Copying core.cpython-311-darwin.so to /Users/rutwik/piper-model-training/piper_repo/src/python/piper_train/vits/monotonic_align...\n",
      "Success: Monotonic alignment module built and installed.\n"
     ]
    }
   ],
   "source": [
    "!python3 setup.py build_ext --inplace\n",
    "\n",
    "# Copy the compiled .so file back to the SOURCE location in piper_repo\n",
    "# This is crucial so the training script can find it later\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Find the compiled file (e.g., core.cpython-311-darwin.so)\n",
    "compiled_files = glob.glob(\"core*.so\")\n",
    "\n",
    "if compiled_files:\n",
    "    src_so = compiled_files[0]\n",
    "    # Destination: piper_train/vits/monotonic_align/ inside your repo\n",
    "    dest_dir = os.path.join(piper_src_path, \"piper_train/vits/monotonic_align\")\n",
    "    \n",
    "    print(f\"Copying {src_so} to {dest_dir}...\")\n",
    "    shutil.copy(src_so, dest_dir)\n",
    "    print(\"Success: Monotonic alignment module built and installed.\")\n",
    "else:\n",
    "    print(\"Error: Build failed, no .so file found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a01f9b",
   "metadata": {},
   "source": [
    "### Checking if the core.so is formed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6g5YbZL60jLg",
    "outputId": "e7006153-4484-460b-8a5c-10558c676607"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing contents of: /Users/rutwik/piper-model-training/piper_repo/src/python/piper_train/vits/monotonic_align\n",
      "Makefile\n",
      "__init__.py\n",
      "core.c\n",
      "setup.py\n",
      "core.cpython-311-darwin.so\n",
      "core.pyx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the correct path to the monotonic_align directory\n",
    "# Assuming 'piper_src_path' is still defined from previous cells as ../piper_repo/src/python\n",
    "piper_src_path = os.path.abspath(\"../../piper_repo/src/python\") # Adjusting for being inside temp_build\n",
    "monotonic_align_dir = os.path.join(piper_src_path, \"piper_train/vits/monotonic_align\")\n",
    "\n",
    "print(f\"Listing contents of: {monotonic_align_dir}\")\n",
    "\n",
    "if os.path.exists(monotonic_align_dir):\n",
    "    files = os.listdir(monotonic_align_dir)\n",
    "    for f in files:\n",
    "        print(f)\n",
    "else:\n",
    "    print(\"Directory not found. Please check the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e9a65",
   "metadata": {},
   "source": [
    "### Checking if the CSV file is Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0mGcSQCeEAxz",
    "outputId": "d481daad-0c28-4e82-bea9-dc21bf64e936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: dataset/english/metadata.csv\n",
      "train_hindifullfemale_00001|Author of the danger trail, Philip Steels, etc.\n",
      "train_hindifullfemale_00002|Not at this particular case, Tom, apologized Whittemore.\n",
      "train_hindifullfemale_00003|For the twentieth time that evening the two men shook hands.\n",
      "train_hindifullfemale_00004|Lord, but I'm glad to see you again, Phil.\n",
      "train_hindifullfemale_00005|Will we ever forget it.\n"
     ]
    }
   ],
   "source": [
    "# 1. Switch back to the main scripts directory\n",
    "\n",
    "import os\n",
    "\n",
    "# 2. Define the correct path relative to 'scripts'\n",
    "dataset_csv_path = \"dataset/english/metadata.csv\"\n",
    "\n",
    "print(f\"Reading: {dataset_csv_path}\")\n",
    "\n",
    "# 3. Read and print the file\n",
    "if os.path.exists(dataset_csv_path):\n",
    "    with open(dataset_csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            print(line.strip())\n",
    "            if i == 4:  # Show only first 5 lines\n",
    "                break\n",
    "else:\n",
    "    print(f\"Error: File not found at {dataset_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {training_dir_ruru}/lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Training on 1 cpu(s)\n",
      "DEBUG:piper_train:Namespace(dataset_dir='training_dir_ruru', checkpoint_epochs=1, quality='medium', resume_from_single_speaker_checkpoint=None, max_epochs=2000, accelerator='cpu', devices='1', precision='32', default_root_dir=None, resume_from_checkpoint='training_dir_ruru/lightning_logs/version_4/checkpoints/epoch=1503-step=3024.ckpt', batch_size=8, validation_split=0.0, num_test_examples=0, num_workers=1, max_phoneme_ids=None, hidden_channels=192, inter_channels=192, filter_channels=768, n_layers=6, n_heads=2, seed=1234)\n",
      "DEBUG:piper_train:Checkpoints will be saved every 1 epoch(s)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "DEBUG:vits.dataset:Loading dataset: training_dir_ruru/dataset.jsonl\n",
      "2026-01-30 05:26:13.855229: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
      "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
      "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
      "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
      "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
      "/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "Restoring states from the checkpoint path at training_dir_ruru/lightning_logs/version_4/checkpoints/epoch=1503-step=3024.ckpt\n",
      "DEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_4/checkpoints/epoch=1503-step=3024.ckpt\n",
      "/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:566: The dirpath has changed from 'training_dir_ruru/lightning_logs/version_4/checkpoints' to 'training_dir_ruru/lightning_logs/version_5/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
      "â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mType                    \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n",
      "â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
      "â”‚\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0mâ”‚ model_g â”‚ SynthesizerTrn           â”‚ 23.7 M â”‚ train â”‚     0 â”‚\n",
      "â”‚\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0mâ”‚ model_d â”‚ MultiPeriodDiscriminator â”‚ 46.7 M â”‚ train â”‚     0 â”‚\n",
      "â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\u001b[1mTrainable params\u001b[0m: 70.4 M                                                        \n",
      "\u001b[1mNon-trainable params\u001b[0m: 0                                                         \n",
      "\u001b[1mTotal params\u001b[0m: 70.4 M                                                            \n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 281                                     \n",
      "\u001b[1mModules in train mode\u001b[0m: 504                                                      \n",
      "\u001b[1mModules in eval mode\u001b[0m: 0                                                         \n",
      "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                  \n",
      "DEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/hparams.yaml\n",
      "Restored all states from the checkpoint at training_dir_ruru/lightning_logs/version_4/checkpoints/epoch=1503-step=3024.ckpt\n",
      "\u001b[2K/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorc\n",
      "h_lightning/trainer/connectors/data_connector.py:429: Consider setting \n",
      "`persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker \n",
      "initialization.\n",
      "\u001b[2K/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorc\n",
      "h_lightning/utilities/data.py:106: Total length of `DataLoader` across ranks is \n",
      "zero. Please make sure this was your intention.\n",
      "\u001b[2K/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorc\n",
      "h_lightning/trainer/connectors/data_connector.py:429: Consider setting \n",
      "`persistent_workers=True` in 'train_dataloader' to speed up the dataloader \n",
      "worker initialization.\n",
      "\u001b[2K/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorc\n",
      "h_lightning/loops/fit_loop.py:317: The number of training batches (3) is smaller\n",
      "than the logging interval Trainer(log_every_n_steps=50). Set a lower value for \n",
      "log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2KEpoch 1504/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:30 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.11it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1504-step=3030.ckpt\n",
      "\u001b[2KEpoch 1505/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:38 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.08it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1505-step=3036.ckpt\n",
      "\u001b[2KEpoch 1506/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:33 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.11it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1506-step=3042.ckpt\n",
      "\u001b[2KEpoch 1507/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:30 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.12it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1507-step=3048.ckpt\n",
      "\u001b[2KEpoch 1508/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:31 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.12it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1508-step=3054.ckpt\n",
      "\u001b[2KEpoch 1509/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:33 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.10it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1509-step=3060.ckpt\n",
      "\u001b[2KEpoch 1510/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:34 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.10it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1510-step=3066.ckpt\n",
      "\u001b[2KEpoch 1511/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:34 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.11it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1511-step=3072.ckpt\n",
      "\u001b[2KEpoch 1512/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:33 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.12it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1512-step=3078.ckpt\n",
      "\u001b[2KEpoch 1513/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:33 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.12it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1513-step=3084.ckpt\n",
      "\u001b[2KEpoch 1514/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:31 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.11it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1514-step=3090.ckpt\n",
      "\u001b[2KEpoch 1515/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:31 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.12it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1515-step=3096.ckpt\n",
      "\u001b[2KEpoch 1516/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:32 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.12it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1516-step=3102.ckpt\n",
      "\u001b[2KEpoch 1517/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:30 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.12it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1517-step=3108.ckpt\n",
      "\u001b[2KEpoch 1518/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:32 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.11it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1518-step=3114.ckpt\n",
      "\u001b[2KEpoch 1519/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:31 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.12it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1519-step=3120.ckpt\n",
      "\u001b[2KEpoch 1520/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:32 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.11it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1520-step=3126.ckpt\n",
      "\u001b[2KEpoch 1521/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:31 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.11it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1521-step=3132.ckpt\n",
      "\u001b[2KEpoch 1522/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:32 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.11it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1522-step=3138.ckpt\n",
      "\u001b[2KEpoch 1523/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:31 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.12it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1523-step=3144.ckpt\n",
      "\u001b[2KEpoch 1524/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:31 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.12it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1524-step=3150.ckpt\n",
      "\u001b[2KEpoch 1525/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:31 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.12it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1525-step=3156.ckpt\n",
      "\u001b[2KEpoch 1526/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/3 \u001b[2m0:00:31 â€¢ 0:00:00\u001b[0m \u001b[2;4m0.12it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m3mv_num: 5.000\u001b[0mDEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir_ruru/lightning_logs/version_5/checkpoints/epoch=1526-step=3162.ckpt\n",
      "\u001b[2KEpoch 1527/1999 \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m 2/3 \u001b[2m0:00:25 â€¢ 0:00:11\u001b[0m \u001b[2;4m0.09it/s\u001b[0m \u001b[3mv_num: 5.000\u001b[0m"
     ]
    }
   ],
   "source": [
    "# @title 8. Start Training\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Move to the scripts directory where the paths are valid relative to\n",
    "os.chdir('/Users/rutwik/piper-model-training/scripts')\n",
    "\n",
    "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "print(f\"ğŸš€ Training on {devices} {accelerator}(s)\")\n",
    "\n",
    "# Note the ../piper_repo path adjustment since we are now in scripts/\n",
    "piper_src_path = os.path.abspath(\"../piper_repo/src/python\")\n",
    "checkpoint_path = \"training_dir_ruru/lightning_logs/version_4/checkpoints/epoch=1503-step=3024.ckpt\"\n",
    "\n",
    "!PYTHONPATH=\"{piper_src_path}\" python -m piper_train \\\n",
    "  --dataset-dir training_dir_ruru \\\n",
    "  --accelerator {accelerator} \\\n",
    "  --devices {devices} \\\n",
    "  --batch-size 8 \\\n",
    "  --validation-split 0.0 \\\n",
    "  --num-test-examples 0 \\\n",
    "  --max_epochs 2000 \\\n",
    "  --resume_from_checkpoint \"{checkpoint_path}\" \\\n",
    "  --checkpoint-epochs 1 \\\n",
    "  --quality medium \\\n",
    "  --precision 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "voice_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Piper TTS Dataset Preprocessing (Google Colab)\n",
                "\n",
                "This notebook preprocesses your audio dataset for Piper TTS training.\n",
                "\n",
                "**Your Dataset**: `/content/drive/MyDrive/english` folder"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Mount Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "# Mount Google Drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Navigate to working directory\n",
                "%cd /content\n",
                "\n",
                "print(f\"\\n‚úÖ Google Drive mounted successfully\")\n",
                "print(f\"Working directory: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Clone Piper Repository"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Clone Piper only if not already present\n",
                "if not os.path.exists('piper_repo'):\n",
                "    print(\"Cloning Piper repository...\")\n",
                "    !git clone https://github.com/rhasspy/piper.git piper_repo\n",
                "    print(\"‚úÖ Repository cloned\")\n",
                "else:\n",
                "    print(\"‚úÖ Piper repository already exists\")\n",
                "\n",
                "piper_src_path = os.path.abspath(\"piper_repo/src/python\")\n",
                "print(f\"Piper source: {piper_src_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Install Build Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üì¶ Installing build dependencies...\\n\")\n",
                "\n",
                "# Install core build tools\n",
                "!pip install -q --upgrade pip setuptools wheel\n",
                "!pip install -q cython numpy pybind11\n",
                "\n",
                "print(\"‚úÖ Build tools installed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Build and Install piper-phonemize from Source\n",
                "\n",
                "Since piper-phonemize isn't on PyPI, we'll build it from the GitHub source."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Clone piper-phonemize repo\n",
                "if not os.path.exists('piper-phonemize'):\n",
                "    print(\"Cloning piper-phonemize repository...\")\n",
                "    !git clone https://github.com/rhasspy/piper-phonemize.git\n",
                "    print(\"‚úÖ Repository cloned\")\n",
                "else:\n",
                "    print(\"‚úÖ piper-phonemize repository already exists\")\n",
                "\n",
                "# Install system dependencies for espeak-ng\n",
                "print(\"\\nInstalling system dependencies...\")\n",
                "!apt-get update -qq\n",
                "!apt-get install -y -qq espeak-ng libespeak-ng-dev\n",
                "\n",
                "# Build and install piper-phonemize\n",
                "print(\"\\nBuilding piper-phonemize from source...\")\n",
                "%cd piper-phonemize\n",
                "!pip install -e .\n",
                "%cd /content\n",
                "\n",
                "print(\"\\n‚úÖ piper-phonemize installed\")\n",
                "\n",
                "# Verify installation\n",
                "try:\n",
                "    import piper_phonemize\n",
                "    print(\"‚úÖ piper_phonemize imported successfully!\")\n",
                "except ImportError as e:\n",
                "    print(f\"‚ùå Import failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Install ML and Audio Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üì¶ Installing ML and audio processing libraries...\\n\")\n",
                "\n",
                "# Install ML frameworks\n",
                "!pip install -q torch>=2.0.0\n",
                "!pip install -q lightning>=2.0.0\n",
                "\n",
                "# Install audio processing\n",
                "!pip install -q librosa<1\n",
                "!pip install -q numba==0.62.1\n",
                "\n",
                "# Install other requirements\n",
                "!pip install -q onnx onnxruntime\n",
                "!pip install -q tensorboard tensorboardX\n",
                "!pip install -q pysilero-vad>=2.1\n",
                "!pip install -q jsonargparse[signatures]>=4.27.7\n",
                "!pip install -q pathvalidate>=3\n",
                "!pip install -q phonemizer Unidecode tqdm inflect matplotlib pandas\n",
                "\n",
                "print(\"\\n‚úÖ All dependencies installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6. Verify Dataset Folder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Your dataset path\n",
                "dataset_path = '/content/drive/MyDrive/english'\n",
                "\n",
                "if os.path.exists(dataset_path):\n",
                "    print(f\"‚úÖ Found dataset folder\")\n",
                "    print(f\"\\nüìÇ Contents:\")\n",
                "    !ls -la \"{dataset_path}\"\n",
                "else:\n",
                "    print(f\"\\n‚ùå ERROR: Dataset folder not found!\")\n",
                "    print(f\"Expected location: {dataset_path}\")\n",
                "    print(f\"\\nPlease check that your audio files are uploaded to this folder in Google Drive.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7. Analyze Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "# Look for transcript file\n",
                "data_file = os.path.join(dataset_path, 'txt.done.data')  # Adjust if your file has a different name\n",
                "\n",
                "if os.path.exists(data_file):\n",
                "    text = open(data_file, 'r', encoding='utf-8', errors='ignore').read()\n",
                "    \n",
                "    print(\"üìä Dataset Preview:\")\n",
                "    print(text[:1000])\n",
                "    \n",
                "    # Analyze vocabulary\n",
                "    vocab = sorted(set(text))\n",
                "    print(f\"\\nüìù Vocabulary size: {len(vocab)} unique characters\")\n",
                "    print(f\"Characters: {vocab[:20]}...\")  # Show first 20\n",
                "else:\n",
                "    print(f\"‚ùå Transcript file not found at: {data_file}\")\n",
                "    print(f\"\\nSearching for text files...\")\n",
                "    !find \"{dataset_path}\" -type f \\( -name \"*.txt\" -o -name \"*.data\" -o -name \"*.csv\" \\) | head -10"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8. Create metadata.csv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Input and output paths\n",
                "metadata_input = os.path.join(dataset_path, \"txt.done.data\")  # Update if your file has a different name\n",
                "metadata_output = os.path.join(dataset_path, \"metadata.csv\")\n",
                "\n",
                "if os.path.exists(metadata_input):\n",
                "    # Read file split by double quotes\n",
                "    df = pd.read_csv(metadata_input, sep='\"', usecols=[0, 1], header=None)\n",
                "    \n",
                "    # Clean filename: remove leading '(' and whitespace\n",
                "    df[0] = df[0].str.replace(r'\\(', '', regex=True).str.strip()\n",
                "    \n",
                "    # Clean transcript\n",
                "    df[1] = df[1].str.strip()\n",
                "    \n",
                "    # Add speaker info (optional)\n",
                "    df[2] = 'female'  # Change to 'male' or your speaker ID\n",
                "    \n",
                "    # Save metadata.csv\n",
                "    df.to_csv(metadata_output, sep='|', index=False, header=False)\n",
                "    \n",
                "    print(f\"‚úÖ Created metadata.csv with {len(df)} entries\")\n",
                "    print(f\"Saved to: {metadata_output}\")\n",
                "    print(f\"\\nPreview:\")\n",
                "    display(df.head())\n",
                "else:\n",
                "    print(f\"‚ùå Input file not found: {metadata_input}\")\n",
                "    print(\"\\nüí° Tip: Update the 'metadata_input' variable to match your transcript file name.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 9. Run Preprocessing\n",
                "\n",
                "This converts audio files and creates training data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Paths\n",
                "dataset_input = '/content/drive/MyDrive/english'\n",
                "piper_src_path = os.path.abspath(\"piper_repo/src/python\")\n",
                "output_dir = \"/content/drive/MyDrive/training_dir\"  # Save to Drive\n",
                "\n",
                "print(f\"Dataset: {dataset_input}\")\n",
                "print(f\"Output: {output_dir}\")\n",
                "print(f\"\\nStarting preprocessing...\\n\")\n",
                "\n",
                "# Run preprocessing\n",
                "!PYTHONPATH=\"{piper_src_path}\" python3 -m piper_train.preprocess \\\n",
                "  --language en \\\n",
                "  --input-dir \"{dataset_input}\" \\\n",
                "  --output-dir \"{output_dir}\" \\\n",
                "  --dataset-format ljspeech \\\n",
                "  --single-speaker \\\n",
                "  --sample-rate 22050\n",
                "\n",
                "print(\"\\n‚úÖ Preprocessing complete!\")\n",
                "print(f\"Training data saved to: {output_dir}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 10. Build Monotonic Align Extension"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import subprocess\n",
                "\n",
                "monotonic_align_path = os.path.join(piper_src_path, \"piper_train/vits/monotonic_align\")\n",
                "\n",
                "print(f\"Building in: {monotonic_align_path}\")\n",
                "\n",
                "try:\n",
                "    subprocess.check_call(\n",
                "        f\"cd '{monotonic_align_path}' && '{sys.executable}' setup.py build_ext --inplace\",\n",
                "        shell=True\n",
                "    )\n",
                "    print(\"\\n‚úÖ Monotonic align built successfully\")\n",
                "except subprocess.CalledProcessError as e:\n",
                "    print(f\"\\n‚ùå Build failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 11. Verify Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import glob\n",
                "\n",
                "# Check if all required files exist\n",
                "checks = [\n",
                "    (\"metadata.csv\", os.path.join(dataset_input, \"metadata.csv\")),\n",
                "    (\"config.json\", os.path.join(output_dir, \"config.json\")),\n",
                "    (\"dataset.jsonl\", os.path.join(output_dir, \"dataset.jsonl\")),\n",
                "    (\"monotonic_align\", os.path.join(monotonic_align_path, \"core*.so\"))\n",
                "]\n",
                "\n",
                "print(\"\\nüîç Verification:\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "all_good = True\n",
                "for name, path in checks:\n",
                "    # Handle wildcard for .so file\n",
                "    if \"*\" in path:\n",
                "        files = glob.glob(path)\n",
                "        exists = len(files) > 0\n",
                "    else:\n",
                "        exists = os.path.exists(path)\n",
                "    \n",
                "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
                "    print(f\"{status} {name}\")\n",
                "    all_good = all_good and exists\n",
                "\n",
                "print(\"=\"*50)\n",
                "if all_good:\n",
                "    print(\"\\nüéâ All checks passed! Ready for training.\")\n",
                "    print(f\"\\nTraining directory: {output_dir}\")\n",
                "    print(\"\\nNext step: Use this training_dir with your training notebook.\")\n",
                "else:\n",
                "    print(\"\\n‚ö†Ô∏è Some checks failed. Review the output above.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 12. Preview Training Data (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show first few lines of dataset.jsonl\n",
                "dataset_jsonl = os.path.join(output_dir, \"dataset.jsonl\")\n",
                "\n",
                "if os.path.exists(dataset_jsonl):\n",
                "    print(\"üìÑ Preview of training data:\\n\")\n",
                "    with open(dataset_jsonl, 'r') as f:\n",
                "        for i, line in enumerate(f):\n",
                "            if i < 3:  # Show first 3 entries\n",
                "                print(line.strip())\n",
                "            else:\n",
                "                break\n",
                "    \n",
                "    # Count total entries\n",
                "    with open(dataset_jsonl, 'r') as f:\n",
                "        total = sum(1 for _ in f)\n",
                "    print(f\"\\nüìä Total training examples: {total}\")\n",
                "else:\n",
                "    print(\"‚ùå dataset.jsonl not found\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Piper TTS Dataset Preprocessing (Google Colab)\n",
                "\n",
                "This notebook preprocesses your audio dataset for Piper TTS training.\n",
                "\n",
                "**Requirements**:\n",
                "- Your dataset ZIP file uploaded to Google Drive\n",
                "- Google Colab with GPU runtime (recommended)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Mount Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "# Mount Google Drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Set working directory to your Google Drive folder\n",
                "drive_folder = '/content/drive/MyDrive/piper-model-training'\n",
                "os.makedirs(drive_folder, exist_ok=True)\n",
                "%cd {drive_folder}\n",
                "\n",
                "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Clone Piper Repository"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Clone Piper only if not already present\n",
                "if not os.path.exists('piper_repo'):\n",
                "    print(\"Cloning Piper repository...\")\n",
                "    !git clone https://github.com/rhasspy/piper.git piper_repo\n",
                "    print(\"‚úÖ Repository cloned\")\n",
                "else:\n",
                "    print(\"‚úÖ Piper repository already exists\")\n",
                "\n",
                "piper_src_path = os.path.abspath(\"piper_repo/src/python\")\n",
                "print(f\"Piper source: {piper_src_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install -q cython numpy pandas matplotlib tensorflow\n",
                "!pip install -q piper-phonemize\n",
                "\n",
                "print(\"‚úÖ Dependencies installed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Extract Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import zipfile\n",
                "import os\n",
                "\n",
                "# TODO: Update this to match your ZIP file name in Google Drive\n",
                "zip_filename = 'hindi_female_english.zip'  # Change this to your actual filename\n",
                "\n",
                "zip_path = os.path.join(drive_folder, zip_filename)\n",
                "output_dir = 'dataset'\n",
                "\n",
                "if os.path.exists(zip_path):\n",
                "    print(f\"üì¶ Found zip file: {zip_filename}\")\n",
                "    print(\"Extracting...\")\n",
                "    \n",
                "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
                "        zip_ref.extractall(output_dir)\n",
                "    \n",
                "    print(f\"‚úÖ Extracted to '{output_dir}/'\")\n",
                "else:\n",
                "    print(f\"\\n‚ùå ERROR: Zip file not found!\")\n",
                "    print(f\"Expected location: {zip_path}\")\n",
                "    print(f\"\\nPlease upload '{zip_filename}' to: {drive_folder}\")\n",
                "    print(\"\\nOr update 'zip_filename' variable in this cell to match your file.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Analyze Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "# TODO: Update this path to match your dataset structure\n",
                "# Common paths: 'dataset/english/txt.done.data' or 'dataset/metadata.txt'\n",
                "data_file = 'dataset/english/txt.done.data'\n",
                "\n",
                "if os.path.exists(data_file):\n",
                "    text = open(data_file, 'r', encoding='utf-8', errors='ignore').read()\n",
                "    \n",
                "    print(\"üìä Dataset Preview:\")\n",
                "    print(text[:1000])\n",
                "    \n",
                "    # Analyze vocabulary\n",
                "    vocab = sorted(set(text))\n",
                "    print(f\"\\nüìù Vocabulary size: {len(vocab)} unique characters\")\n",
                "    print(f\"Characters: {vocab[:20]}...\")  # Show first 20\n",
                "else:\n",
                "    print(f\"‚ùå Data file not found at: {data_file}\")\n",
                "    print(f\"\\nAvailable files in dataset:\")\n",
                "    !ls -la dataset/"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6. Create metadata.csv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# TODO: Update paths based on your dataset structure\n",
                "metadata_input = \"dataset/english/txt.done.data\"\n",
                "metadata_output = \"dataset/english/metadata.csv\"\n",
                "\n",
                "if os.path.exists(metadata_input):\n",
                "    # Read file split by double quotes\n",
                "    df = pd.read_csv(metadata_input, sep='\"', usecols=[0, 1], header=None)\n",
                "    \n",
                "    # Clean filename: remove leading '(' and whitespace\n",
                "    df[0] = df[0].str.replace(r'\\(', '', regex=True).str.strip()\n",
                "    \n",
                "    # Clean transcript\n",
                "    df[1] = df[1].str.strip()\n",
                "    \n",
                "    # Add speaker info (optional)\n",
                "    df[2] = 'female'  #  Change to 'male' or speaker ID as needed\n",
                "    \n",
                "    # Save metadata.csv\n",
                "    df.to_csv(metadata_output, sep='|', index=False, header=False)\n",
                "    \n",
                "    print(f\"‚úÖ Created metadata.csv with {len(df)} entries\")\n",
                "    print(f\"\\nPreview:\")\n",
                "    display(df.head())\n",
                "else:\n",
                "    print(f\"‚ùå Input file not found: {metadata_input}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7. Run Preprocessing\n",
                "\n",
                "This converts audio files and creates training data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Define paths\n",
                "dataset_path = os.path.abspath(\"dataset/english\")  # Update if different\n",
                "piper_src_path = os.path.abspath(\"piper_repo/src/python\")\n",
                "output_dir = \"training_dir\"\n",
                "\n",
                "print(f\"Dataset: {dataset_path}\")\n",
                "print(f\"Output: {output_dir}\")\n",
                "print(f\"\\nStarting preprocessing...\\n\")\n",
                "\n",
                "# Run preprocessing\n",
                "!PYTHONPATH=\"{piper_src_path}\" python3 -m piper_train.preprocess \\\n",
                "  --language en \\\n",
                "  --input-dir \"{dataset_path}\" \\\n",
                "  --output-dir \"{output_dir}\" \\\n",
                "  --dataset-format ljspeech \\\n",
                "  --single-speaker \\\n",
                "  --sample-rate 22050\n",
                "\n",
                "print(\"\\n‚úÖ Preprocessing complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8. Build Monotonic Align Extension"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import subprocess\n",
                "\n",
                "monotonic_align_path = os.path.join(piper_src_path, \"piper_train/vits/monotonic_align\")\n",
                "\n",
                "print(f\"Building in: {monotonic_align_path}\")\n",
                "\n",
                "try:\n",
                "    subprocess.check_call(\n",
                "        f\"cd '{monotonic_align_path}' && '{sys.executable}' setup.py build_ext --inplace\",\n",
                "        shell=True\n",
                "    )\n",
                "    print(\"\\n‚úÖ Monotonic align built successfully\")\n",
                "except subprocess.CalledProcessError as e:\n",
                "    print(f\"\\n‚ùå Build failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 9. Verify Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if all required files exist\n",
                "checks = [\n",
                "    (\"metadata.csv\", \"dataset/english/metadata.csv\"),\n",
                "    (\"config.json\", \"training_dir/config.json\"),\n",
                "    (\"dataset.jsonl\", \"training_dir/dataset.jsonl\"),\n",
                "    (\"monotonic_align\", os.path.join(monotonic_align_path, \"core*.so\"))\n",
                "]\n",
                "\n",
                "print(\"\\nüîç Verification:\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "all_good = True\n",
                "for name, path in checks:\n",
                "    # Handle wildcard for .so file\n",
                "    if \"*\" in path:\n",
                "        import glob\n",
                "        files = glob.glob(path)\n",
                "        exists = len(files) > 0\n",
                "    else:\n",
                "        exists = os.path.exists(path)\n",
                "    \n",
                "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
                "    print(f\"{status} {name}\")\n",
                "    all_good = all_good and exists\n",
                "\n",
                "print(\"=\"*50)\n",
                "if all_good:\n",
                "    print(\"\\nüéâ All checks passed! Ready for training.\")\n",
                "    print(\"\\nNext step: Run the training notebook or copy training_dir to your local machine.\")\n",
                "else:\n",
                "    print(\"\\n‚ö†Ô∏è Some checks failed. Review the output above.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 10. Preview Training Data (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show first few lines of dataset.jsonl\n",
                "dataset_jsonl = \"training_dir/dataset.jsonl\"\n",
                "\n",
                "if os.path.exists(dataset_jsonl):\n",
                "    print(\"üìÑ Preview of training data:\\n\")\n",
                "    with open(dataset_jsonl, 'r') as f:\n",
                "        for i, line in enumerate(f):\n",
                "            if i < 3:  # Show first 3 entries\n",
                "                print(line.strip())\n",
                "            else:\n",
                "                break\n",
                "    \n",
                "    # Count total entries\n",
                "    with open(dataset_jsonl, 'r') as f:\n",
                "        total = sum(1 for _ in f)\n",
                "    print(f\"\\nüìä Total training examples: {total}\")\n",
                "else:\n",
                "    print(\"‚ùå dataset.jsonl not found\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Piper TTS Training on Google Colab (Fixed)\n",
                "\n",
                "This notebook trains a Piper TTS model using a custom dataset on Google Drive.\n",
                "It includes fixes for:\n",
                "1.  **Dependencies**: Compiles `piper-phonemize` and `espeak-ng` from source to avoid Python 3.12 incompatibility.\n",
                "2.  **Permissions**: Builds in `/content` to avoid Google Drive `noexec` permission errors.\n",
                "3.  **PyTorch Lightning**: Compatible with Lightning 2.x via patches."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1. Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2. Install Python Dependencies (PyTorch < 2.6)\n",
                "# We strictly use PyTorch 2.5.1 to avoid the new 'weights_only=True' default in 2.6\n",
                "# which breaks loading legacy Piper checkpoints (pathlib issues).\n",
                "!pip install -q torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1\n",
                "!pip install -q lightning==2.4.0\n",
                "!pip install -q librosa<1 numba==0.62.1\n",
                "!pip install -q onnx onnxruntime tensorboard tensorboardX\n",
                "!pip install -q pysilero-vad>=2.1 pathvalidate>=3\n",
                "!pip install -q phonemizer Unidecode tqdm inflect matplotlib pandas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 3. The \"God Script\" (Build Dependencies from Source)\n",
                "# This script handles EVERYTHING related to building espeak-ng and piper-phonemize.\n",
                "# It avoids Google Drive permission issues by building in /content.\n",
                "\n",
                "import os\n",
                "import shutil\n",
                "import subprocess\n",
                "\n",
                "# Define build directories in /content (NOT Drive)\n",
                "BUILD_DIR = \"/content/build_temp\"\n",
                "if os.path.exists(BUILD_DIR):\n",
                "    shutil.rmtree(BUILD_DIR)\n",
                "os.makedirs(BUILD_DIR)\n",
                "\n",
                "print(\"\ud83d\ude80 Starting God Script: Building dependencies in /content...\")\n",
                "\n",
                "# 1. Install System Dependencies\n",
                "print(\"\ud83d\udce6 Installing system dependencies...\")\n",
                "!sudo apt-get update -qq\n",
                "!sudo apt-get install -y -qq build-essential cmake git autoconf automake libtool pkg-config\n",
                "\n",
                "# 2. Build & Install Rhasspy's espeak-ng Fork\n",
                "print(\"\u2b07\ufe0f Cloning rhasspy/espeak-ng...\")\n",
                "!cd {BUILD_DIR} && git clone https://github.com/rhasspy/espeak-ng.git\n",
                "\n",
                "print(\"\ud83d\udd28 Building espeak-ng... (This takes ~2 mins)\")\n",
                "# We force standard paths to ensure linker finds them later\n",
                "!cd {BUILD_DIR}/espeak-ng && ./autogen.sh && ./configure --prefix=/usr --without-async --without-mbrola --without-sonic && make -j4 && sudo make install\n",
                "print(\"\u2705 espeak-ng installed!\")\n",
                "\n",
                "# 3. Build piper-phonemize (The tricky part)\n",
                "print(\"\u2b07\ufe0f Cloning piper-phonemize...\")\n",
                "!cd {BUILD_DIR} && git clone https://github.com/rhasspy/piper-phonemize.git\n",
                "\n",
                "# Download ONNX Runtime (needed for headers)\n",
                "print(\"\u2b07\ufe0f Downloading ONNX Runtime...\")\n",
                "onnx_ver = \"1.14.1\"\n",
                "onnx_file = f\"onnxruntime-linux-x64-{onnx_ver}.tgz\"\n",
                "!cd {BUILD_DIR}/piper-phonemize && wget -q https://github.com/microsoft/onnxruntime/releases/download/v{onnx_ver}/{onnx_file}\n",
                "!cd {BUILD_DIR}/piper-phonemize && tar -xzf {onnx_file}\n",
                "!mkdir -p {BUILD_DIR}/piper-phonemize/lib/Linux-x86_64/onnxruntime/include\n",
                "!mkdir -p {BUILD_DIR}/piper-phonemize/lib/Linux-x86_64/onnxruntime/lib\n",
                "!cp -r {BUILD_DIR}/piper-phonemize/onnxruntime-linux-x64-{onnx_ver}/include/* {BUILD_DIR}/piper-phonemize/lib/Linux-x86_64/onnxruntime/include/\n",
                "!cp -r {BUILD_DIR}/piper-phonemize/onnxruntime-linux-x64-{onnx_ver}/lib/* {BUILD_DIR}/piper-phonemize/lib/Linux-x86_64/onnxruntime/lib/\n",
                "\n",
                "# Install Python build tools\n",
                "!pip install -q cython numpy pybind11 setuptools wheel\n",
                "\n",
                "# 4. Compile Python Extension\n",
                "print(\"\ud83d\udc0d Compiling piper_phonemize python extension...\")\n",
                "# We MUST force the build to look in /usr/include for espeak-ng headers\n",
                "!cd {BUILD_DIR}/piper-phonemize && python setup.py build_ext --inplace\n",
                "!cd {BUILD_DIR}/piper-phonemize && python setup.py install\n",
                "\n",
                "print(\"\u2705 piper-phonemize installed!\")\n",
                "\n",
                "# 5. Runtime Library Fix (Crucial!)\n",
                "print(\"\ud83d\udd27 Applying Runtime Fixes...\")\n",
                "src_lib = f\"{BUILD_DIR}/piper-phonemize/lib/Linux-x86_64/onnxruntime/lib/libonnxruntime.so.{onnx_ver}\"\n",
                "!sudo cp {src_lib} /usr/lib/libonnxruntime.so.{onnx_ver}\n",
                "!sudo ln -fs /usr/lib/libonnxruntime.so.{onnx_ver} /usr/lib/libonnxruntime.so\n",
                "!sudo ldconfig\n",
                "\n",
                "print(\"\ud83c\udf89 GOD SCRIPT COMPLETE! Dependencies are ready.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 4. Clone Piper Repository & Patch Code\n",
                "import os\n",
                "\n",
                "# CLONE REPO\n",
                "if not os.path.exists(\"piper_repo\"):\n",
                "    !git clone https://github.com/rhasspy/piper.git piper_repo\n",
                "    print(\"\u2705 Cloned piper repository\")\n",
                "else:\n",
                "    print(\"\u2705 piper_repo already exists\")\n",
                "\n",
                "# --- PATCHING ---\n",
                "print(\"\\n\ud83e\ude79 Applying PyTorch Lightning 2.x Patches...\")\n",
                "\n",
                "# 1. Fix __main__.py (Trainer compatibility)\n",
                "main_py_path = \"piper_repo/src/python/piper_train/__main__.py\"\n",
                "main_py_content = \"\"\"import argparse\nimport json\nimport logging\nfrom pathlib import Path\n\nimport torch\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\nfrom .vits.lightning import VitsModel\n\n_LOGGER = logging.getLogger(__package__)\n\n\ndef main():\n    logging.basicConfig(level=logging.DEBUG)\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--dataset-dir\", required=True, help=\"Path to pre-processed dataset directory\"\n    )\n    parser.add_argument(\n        \"--checkpoint-epochs\",\n        type=int,\n        help=\"Save checkpoint every N epochs (default: 1)\",\n    )\n    parser.add_argument(\n        \"--quality\",\n        default=\"medium\",\n        choices=(\"x-low\", \"medium\", \"high\"),\n        help=\"Quality/size of model (default: medium)\",\n    )\n    parser.add_argument(\n        \"--resume_from_single_speaker_checkpoint\",\n        help=\"For multi-speaker models only. Converts a single-speaker checkpoint to multi-speaker and resumes training\",\n    )\n    \n    # Manually add PL 2.x arguments that we use\n    parser.add_argument(\"--max_epochs\", type=int, default=1000)\n    parser.add_argument(\"--accelerator\", default=\"auto\")\n    parser.add_argument(\"--devices\", default=\"auto\")\n    parser.add_argument(\"--precision\", default=\"32-true\")\n    parser.add_argument(\"--default_root_dir\", type=str, default=None)\n    parser.add_argument(\"--resume_from_checkpoint\", type=str, default=None)\n\n    # Trainer.add_argparse_args(parser) # Removed in PL 2.0\n    VitsModel.add_model_specific_args(parser)\n    parser.add_argument(\"--seed\", type=int, default=1234)\n    args = parser.parse_args()\n    _LOGGER.debug(args)\n\n    args.dataset_dir = Path(args.dataset_dir)\n    if not args.default_root_dir:\n        args.default_root_dir = str(args.dataset_dir) # Must be string for Trainer explicitly\n    \n    torch.backends.cudnn.benchmark = True\n    torch.manual_seed(args.seed)\n\n    config_path = args.dataset_dir / \"config.json\"\n    dataset_path = args.dataset_dir / \"dataset.jsonl\"\n\n    with open(config_path, \"r\", encoding=\"utf-8\") as config_file:\n        config = json.load(config_file)\n        num_symbols = int(config[\"num_symbols\"])\n        num_speakers = int(config[\"num_speakers\"])\n        sample_rate = int(config[\"audio\"][\"sample_rate\"])\n\n\n    callbacks = []\n    if args.checkpoint_epochs is not None:\n        callbacks = [ModelCheckpoint(every_n_epochs=args.checkpoint_epochs)]\n        _LOGGER.debug(\n            \"Checkpoints will be saved every %s epoch(s)\", args.checkpoint_epochs\n        )\n\n    # Instantiate Trainer explicitly\n    trainer = Trainer(\n        max_epochs=args.max_epochs,\n        accelerator=args.accelerator,\n        devices=int(args.devices) if isinstance(args.devices, str) and args.devices.isdigit() else args.devices,\n        precision=args.precision,\n        default_root_dir=args.default_root_dir,\n        callbacks=callbacks\n    )\n\n    dict_args = vars(args)\n    if args.quality == \"x-low\":\n        dict_args[\"hidden_channels\"] = 96\n        dict_args[\"inter_channels\"] = 96\n        dict_args[\"filter_channels\"] = 384\n    elif args.quality == \"high\":\n        dict_args[\"resblock\"] = \"1\"\n        dict_args[\"resblock_kernel_sizes\"] = (3, 7, 11)\n        dict_args[\"resblock_dilation_sizes\"] = (\n            (1, 3, 5),\n            (1, 3, 5),\n            (1, 3, 5),\n        )\n        dict_args[\"upsample_rates\"] = (8, 8, 2, 2)\n        dict_args[\"upsample_initial_channel\"] = 512\n        dict_args[\"upsample_kernel_sizes\"] = (16, 16, 4, 4)\n\n    model = VitsModel(\n        num_symbols=num_symbols,\n        num_speakers=num_speakers,\n        sample_rate=sample_rate,\n        dataset=[dataset_path],\n        **dict_args,\n    )\n\n    if args.resume_from_single_speaker_checkpoint:\n        assert (\n            num_speakers > 1\n        ), \"--resume_from_single_speaker_checkpoint is only for multi-speaker models. Use --resume_from_checkpoint for single-speaker models.\"\n\n        # Load single-speaker checkpoint\n        _LOGGER.debug(\n            \"Resuming from single-speaker checkpoint: %s\",\n            args.resume_from_single_speaker_checkpoint,\n        )\n        model_single = VitsModel.load_from_checkpoint(\n            args.resume_from_single_speaker_checkpoint,\n            dataset=None,\n        )\n        g_dict = model_single.model_g.state_dict()\n        for key in list(g_dict.keys()):\n            # Remove keys that can't be copied over due to missing speaker embedding\n            if (\n                key.startswith(\"dec.cond\")\n                or key.startswith(\"dp.cond\")\n                or (\"enc.cond_layer\" in key)\n            ):\n                g_dict.pop(key, None)\n\n        # Copy over the multi-speaker model, excluding keys related to the\n        # speaker embedding (which is missing from the single-speaker model).\n        load_state_dict(model.model_g, g_dict)\n        load_state_dict(model.model_d, model_single.model_d.state_dict())\n        _LOGGER.info(\n            \"Successfully converted single-speaker checkpoint to multi-speaker\"\n        )\n\n    ckpt_path = args.resume_from_checkpoint\n    if args.resume_from_single_speaker_checkpoint:\n        ckpt_path = None # We manually loaded weights, start fresh\n\n    trainer.fit(model, ckpt_path=ckpt_path)\n\n\ndef load_state_dict(model, saved_state_dict):\n    state_dict = model.state_dict()\n    new_state_dict = {}\n\n    for k, v in state_dict.items():\n        if k in saved_state_dict:\n            # Use saved value\n            new_state_dict[k] = saved_state_dict[k]\n        else:\n            # Use initialized value\n            _LOGGER.debug(\"%s is not in the checkpoint\", k)\n            new_state_dict[k] = v\n\n    model.load_state_dict(new_state_dict)\n\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\n",
                "with open(main_py_path, \"w\") as f:\n",
                "    f.write(main_py_content)\n",
                "print(\"\u2705 Fixed piper_train/__main__.py\")\n",
                "\n",
                "# 2. Fix lightning.py (Manual Optimization)\n",
                "lightning_py_path = \"piper_repo/src/python/piper_train/vits/lightning.py\"\n",
                "lightning_py_content = \"\"\"import logging\nfrom pathlib import Path\nfrom typing import List, Optional, Tuple, Union\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import autocast\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\nfrom .commons import slice_segments\nfrom .dataset import Batch, PiperDataset, UtteranceCollate\nfrom .losses import discriminator_loss, feature_loss, generator_loss, kl_loss\nfrom .mel_processing import mel_spectrogram_torch, spec_to_mel_torch\nfrom .models import MultiPeriodDiscriminator, SynthesizerTrn\n\n_LOGGER = logging.getLogger(\"vits.lightning\")\n\n\nclass VitsModel(pl.LightningModule):\n    def __init__(\n        self,\n        num_symbols: int,\n        num_speakers: int,\n        # audio\n        resblock=\"2\",\n        resblock_kernel_sizes=(3, 5, 7),\n        resblock_dilation_sizes=(\n            (1, 2),\n            (2, 6),\n            (3, 12),\n        ),\n        upsample_rates=(8, 8, 4),\n        upsample_initial_channel=256,\n        upsample_kernel_sizes=(16, 16, 8),\n        # mel\n        filter_length: int = 1024,\n        hop_length: int = 256,\n        win_length: int = 1024,\n        mel_channels: int = 80,\n        sample_rate: int = 22050,\n        sample_bytes: int = 2,\n        channels: int = 1,\n        mel_fmin: float = 0.0,\n        mel_fmax: Optional[float] = None,\n        # model\n        inter_channels: int = 192,\n        hidden_channels: int = 192,\n        filter_channels: int = 768,\n        n_heads: int = 2,\n        n_layers: int = 6,\n        kernel_size: int = 3,\n        p_dropout: float = 0.1,\n        n_layers_q: int = 3,\n        use_spectral_norm: bool = False,\n        gin_channels: int = 0,\n        use_sdp: bool = True,\n        segment_size: int = 8192,\n        # training\n        dataset: Optional[List[Union[str, Path]]] = None,\n        learning_rate: float = 2e-4,\n        betas: Tuple[float, float] = (0.8, 0.99),\n        eps: float = 1e-9,\n        batch_size: int = 1,\n        lr_decay: float = 0.999875,\n        init_lr_ratio: float = 1.0,\n        warmup_epochs: int = 0,\n        c_mel: int = 45,\n        c_kl: float = 1.0,\n        grad_clip: Optional[float] = None,\n        num_workers: int = 1,\n        seed: int = 1234,\n        num_test_examples: int = 5,\n        validation_split: float = 0.1,\n        max_phoneme_ids: Optional[int] = None,\n        **kwargs,\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Lightning 2.x requires manual optimization for multiple optimizers\n        self.automatic_optimization = False\n\n        if (self.hparams.num_speakers > 1) and (self.hparams.gin_channels <= 0):\n            # Default gin_channels for multi-speaker model\n            self.hparams.gin_channels = 512\n\n        # Set up models\n        self.model_g = SynthesizerTrn(\n            n_vocab=self.hparams.num_symbols,\n            spec_channels=self.hparams.filter_length // 2 + 1,\n            segment_size=self.hparams.segment_size // self.hparams.hop_length,\n            inter_channels=self.hparams.inter_channels,\n            hidden_channels=self.hparams.hidden_channels,\n            filter_channels=self.hparams.filter_channels,\n            n_heads=self.hparams.n_heads,\n            n_layers=self.hparams.n_layers,\n            kernel_size=self.hparams.kernel_size,\n            p_dropout=self.hparams.p_dropout,\n            resblock=self.hparams.resblock,\n            resblock_kernel_sizes=self.hparams.resblock_kernel_sizes,\n            resblock_dilation_sizes=self.hparams.resblock_dilation_sizes,\n            upsample_rates=self.hparams.upsample_rates,\n            upsample_initial_channel=self.hparams.upsample_initial_channel,\n            upsample_kernel_sizes=self.hparams.upsample_kernel_sizes,\n            n_speakers=self.hparams.num_speakers,\n            gin_channels=self.hparams.gin_channels,\n            use_sdp=self.hparams.use_sdp,\n        )\n        self.model_d = MultiPeriodDiscriminator(\n            use_spectral_norm=self.hparams.use_spectral_norm\n        )\n\n        # Dataset splits\n        self._train_dataset: Optional[Dataset] = None\n        self._val_dataset: Optional[Dataset] = None\n        self._test_dataset: Optional[Dataset] = None\n        self._load_datasets(validation_split, num_test_examples, max_phoneme_ids)\n\n        # State kept between training optimizers\n        self._y = None\n        self._y_hat = None\n\n    def _load_datasets(\n        self,\n        validation_split: float,\n        num_test_examples: int,\n        max_phoneme_ids: Optional[int] = None,\n    ):\n        if self.hparams.dataset is None:\n            _LOGGER.debug(\"No dataset to load\")\n            return\n\n        full_dataset = PiperDataset(\n            self.hparams.dataset, max_phoneme_ids=max_phoneme_ids\n        )\n        valid_set_size = int(len(full_dataset) * validation_split)\n        train_set_size = len(full_dataset) - valid_set_size - num_test_examples\n\n        self._train_dataset, self._test_dataset, self._val_dataset = random_split(\n            full_dataset, [train_set_size, num_test_examples, valid_set_size]\n        )\n\n    def forward(self, text, text_lengths, scales, sid=None):\n        noise_scale = scales[0]\n        length_scale = scales[1]\n        noise_scale_w = scales[2]\n        audio, *_ = self.model_g.infer(\n            text,\n            text_lengths,\n            noise_scale=noise_scale,\n            length_scale=length_scale,\n            noise_scale_w=noise_scale_w,\n            sid=sid,\n        )\n\n        return audio\n\n    def train_dataloader(self):\n        return DataLoader(\n            self._train_dataset,\n            collate_fn=UtteranceCollate(\n                is_multispeaker=self.hparams.num_speakers > 1,\n                segment_size=self.hparams.segment_size,\n            ),\n            num_workers=self.hparams.num_workers,\n            batch_size=self.hparams.batch_size,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self._val_dataset,\n            collate_fn=UtteranceCollate(\n                is_multispeaker=self.hparams.num_speakers > 1,\n                segment_size=self.hparams.segment_size,\n            ),\n            num_workers=self.hparams.num_workers,\n            batch_size=self.hparams.batch_size,\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self._test_dataset,\n            collate_fn=UtteranceCollate(\n                is_multispeaker=self.hparams.num_speakers > 1,\n                segment_size=self.hparams.segment_size,\n            ),\n            num_workers=self.hparams.num_workers,\n            batch_size=self.hparams.batch_size,\n        )\n\n    def training_step(self, batch: Batch, batch_idx: int):\n        # Manual optimization for Lightning 2.x with multiple optimizers\n        opt_g, opt_d = self.optimizers()\n        \n        # Train Generator\n        loss_gen_all = self.training_step_g(batch)\n        opt_g.zero_grad()\n        self.manual_backward(loss_gen_all)\n        opt_g.step()\n        \n        # Train Discriminator\n        loss_disc_all = self.training_step_d(batch)\n        opt_d.zero_grad()\n        self.manual_backward(loss_disc_all)\n        opt_d.step()\n        \n        # Step learning rate schedulers\n        sch_g, sch_d = self.lr_schedulers()\n        if self.trainer.is_last_batch:\n             sch_g.step()\n             sch_d.step()\n\n\n    def training_step_g(self, batch: Batch):\n        x, x_lengths, y, _, spec, spec_lengths, speaker_ids = (\n            batch.phoneme_ids,\n            batch.phoneme_lengths,\n            batch.audios,\n            batch.audio_lengths,\n            batch.spectrograms,\n            batch.spectrogram_lengths,\n            batch.speaker_ids if batch.speaker_ids is not None else None,\n        )\n        (\n            y_hat,\n            l_length,\n            _attn,\n            ids_slice,\n            _x_mask,\n            z_mask,\n            (_z, z_p, m_p, logs_p, _m_q, logs_q),\n        ) = self.model_g(x, x_lengths, spec, spec_lengths, speaker_ids)\n        self._y_hat = y_hat\n\n        mel = spec_to_mel_torch(\n            spec,\n            self.hparams.filter_length,\n            self.hparams.mel_channels,\n            self.hparams.sample_rate,\n            self.hparams.mel_fmin,\n            self.hparams.mel_fmax,\n        )\n        y_mel = slice_segments(\n            mel,\n            ids_slice,\n            self.hparams.segment_size // self.hparams.hop_length,\n        )\n        y_hat_mel = mel_spectrogram_torch(\n            y_hat.squeeze(1),\n            self.hparams.filter_length,\n            self.hparams.mel_channels,\n            self.hparams.sample_rate,\n            self.hparams.hop_length,\n            self.hparams.win_length,\n            self.hparams.mel_fmin,\n            self.hparams.mel_fmax,\n        )\n        y = slice_segments(\n            y,\n            ids_slice * self.hparams.hop_length,\n            self.hparams.segment_size,\n        )  # slice\n\n        # Save for training_step_d\n        self._y = y\n\n        _y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = self.model_d(y, y_hat)\n\n        with autocast(self.device.type, enabled=False):\n            # Generator loss\n            loss_dur = torch.sum(l_length.float())\n            loss_mel = F.l1_loss(y_mel, y_hat_mel) * self.hparams.c_mel\n            loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * self.hparams.c_kl\n\n            loss_fm = feature_loss(fmap_r, fmap_g)\n            loss_gen, _losses_gen = generator_loss(y_d_hat_g)\n            loss_gen_all = loss_gen + loss_fm + loss_mel + loss_dur + loss_kl\n\n            self.log(\"loss_gen_all\", loss_gen_all, prog_bar=True)\n\n            return loss_gen_all\n\n    def training_step_d(self, batch: Batch):\n        # From training_step_g\n        y = self._y\n        y_hat = self._y_hat\n        y_d_hat_r, y_d_hat_g, _, _ = self.model_d(y, y_hat.detach())\n\n        with autocast(self.device.type, enabled=False):\n            # Discriminator\n            loss_disc, _losses_disc_r, _losses_disc_g = discriminator_loss(\n                y_d_hat_r, y_d_hat_g\n            )\n            loss_disc_all = loss_disc\n\n            self.log(\"loss_disc_all\", loss_disc_all, prog_bar=True)\n\n            return loss_disc_all\n\n    def validation_step(self, batch: Batch, batch_idx: int):\n        val_loss = self.training_step_g(batch) + self.training_step_d(batch)\n        self.log(\"val_loss\", val_loss, prog_bar=True)\n\n        # Generate audio examples\n        for utt_idx, test_utt in enumerate(self._test_dataset):\n            text = test_utt.phoneme_ids.unsqueeze(0).to(self.device)\n            text_lengths = torch.LongTensor([len(test_utt.phoneme_ids)]).to(self.device)\n            scales = [0.667, 1.0, 0.8]\n            sid = (\n                test_utt.speaker_id.to(self.device)\n                if test_utt.speaker_id is not None\n                else None\n            )\n            test_audio = self(text, text_lengths, scales, sid=sid).detach()\n\n            # Scale to make louder in [-1, 1]\n            test_audio = test_audio * (1.0 / max(0.01, abs(test_audio.max())))\n\n            tag = test_utt.text or str(utt_idx)\n            self.logger.experiment.add_audio(\n                tag, test_audio, sample_rate=self.hparams.sample_rate\n            )\n\n        return val_loss\n\n    def configure_optimizers(self):\n        optimizers = [\n            torch.optim.AdamW(\n                self.model_g.parameters(),\n                lr=self.hparams.learning_rate,\n                betas=self.hparams.betas,\n                eps=self.hparams.eps,\n            ),\n            torch.optim.AdamW(\n                self.model_d.parameters(),\n                lr=self.hparams.learning_rate,\n                betas=self.hparams.betas,\n                eps=self.hparams.eps,\n            ),\n        ]\n        schedulers = [\n            torch.optim.lr_scheduler.ExponentialLR(\n                optimizers[0], gamma=self.hparams.lr_decay\n            ),\n            torch.optim.lr_scheduler.ExponentialLR(\n                optimizers[1], gamma=self.hparams.lr_decay\n            ),\n        ]\n\n        return [\n            {\"optimizer\": optimizers[0], \"lr_scheduler\": schedulers[0]},\n            {\"optimizer\": optimizers[1], \"lr_scheduler\": schedulers[1]},\n        ]\n\n    @staticmethod\n    def add_model_specific_args(parent_parser):\n        parser = parent_parser.add_argument_group(\"VitsModel\")\n        parser.add_argument(\"--batch-size\", type=int, required=True)\n        parser.add_argument(\"--validation-split\", type=float, default=0.1)\n        parser.add_argument(\"--num-test-examples\", type=int, default=5)\n        parser.add_argument(\n            \"--max-phoneme-ids\",\n            type=int,\n            help=\"Exclude utterances with phoneme id lists longer than this\",\n        )\n        #\n        parser.add_argument(\"--hidden-channels\", type=int, default=192)\n        parser.add_argument(\"--inter-channels\", type=int, default=192)\n        parser.add_argument(\"--filter-channels\", type=int, default=768)\n        parser.add_argument(\"--n-layers\", type=int, default=6)\n        parser.add_argument(\"--n-heads\", type=int, default=2)\n        #\n        return parent_parser\n\"\"\"\n",
                "with open(lightning_py_path, \"w\") as f:\n",
                "    f.write(lightning_py_content)\n",
                "print(\"\u2705 Fixed piper_train/vits/lightning.py\")\n",
                "\n",
                "# 3. Fix monotonic_align/__init__.py\n",
                "monotonic_init = \"piper_repo/src/python/piper_train/vits/monotonic_align/__init__.py\"\n",
                "if os.path.exists(monotonic_init):\n",
                "    with open(monotonic_init, \"r\") as f:\n",
                "        content = f.read()\n",
                "    content = content.replace(\"from .monotonic_align.core\", \"from .core\")\n",
                "    with open(monotonic_init, \"w\") as f:\n",
                "        f.write(content)\n",
                "    print(\"\u2705 Fixed monotonic_align import\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 5. Build Monotonic Align (Required for VITS)\n",
                "import os\n",
                "import shutil\n",
                "\n",
                "piper_src_path = os.path.abspath(\"piper_repo/src/python\")\n",
                "monotonic_align_src = os.path.join(piper_src_path, \"piper_train/vits/monotonic_align\")\n",
                "\n",
                "# Build directly in place since we have full permissions usually\n",
                "# But to be safe and clean, use temp\n",
                "temp_build_dir = \"/content/monotonic_align_build\"\n",
                "if os.path.exists(temp_build_dir):\n",
                "    shutil.rmtree(temp_build_dir)\n",
                "os.makedirs(temp_build_dir, exist_ok=True)\n",
                "\n",
                "for filename in [\"core.pyx\", \"setup.py\"]:\n",
                "    shutil.copy(os.path.join(monotonic_align_src, filename), os.path.join(temp_build_dir, filename))\n",
                "\n",
                "%cd {temp_build_dir}\n",
                "print(\"\ud83d\udd28 Building monotonic_align...\")\n",
                "!python setup.py build_ext --inplace\n",
                "\n",
                "import glob\n",
                "so_files = glob.glob(\"core*.so\")\n",
                "if so_files:\n",
                "    dest = os.path.join(monotonic_align_src, so_files[0])\n",
                "    shutil.copy(so_files[0], dest)\n",
                "    print(f\"\u2705 Installed compiled extension to: {dest}\")\n",
                "else:\n",
                "    print(\"\u274c Build failed, no .so file found\")\n",
                "\n",
                "%cd /content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 6. Run Preprocessing\n",
                "import os\n",
                "import sys\n",
                "\n",
                "piper_src_path = os.path.abspath(\"piper_repo/src/python\")\n",
                "DATASET_PATH = \"/content/drive/MyDrive/english\" # Ensure this matches your uploaded folder name\n",
                "OUTPUT_DIR = \"/content/drive/MyDrive/piper-model-training/training_dir\"\n",
                "\n",
                "print(f\"\ud83d\udcc2 Dataset: {DATASET_PATH}\")\n",
                "print(f\"\ud83d\udcc2 Output: {OUTPUT_DIR}\")\n",
                "\n",
                "# Add piper_phonemize to path if needed\n",
                "\n",
                "!PYTHONPATH=\"{piper_src_path}\" python -m piper_train.preprocess \\\n",
                "  --language en \\\n",
                "  --input-dir \"{DATASET_PATH}\" \\\n",
                "  --output-dir \"{OUTPUT_DIR}\" \\\n",
                "  --dataset-format ljspeech \\\n",
                "  --single-speaker \\\n",
                "  --sample-rate 22050\n",
                "\n",
                "print(\"\\n\u2705 Preprocessing complete (if no errors above)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 7. Download Base Checkpoint\n",
                "import urllib.request\n",
                "\n",
                "os.makedirs(\"/content/drive/MyDrive/piper-model-training/checkpoints\", exist_ok=True)\n",
                "checkpoint_url = \"https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/en/en_US/lessac/medium/epoch%3D2164-step%3D1355540.ckpt\"\n",
                "checkpoint_path = \"/content/drive/MyDrive/piper-model-training/checkpoints/epoch=2164-step=1355540.ckpt\"\n",
                "\n",
                "if not os.path.exists(checkpoint_path):\n",
                "    print(\"\ud83d\udce5 Downloading base checkpoint...\")\n",
                "    urllib.request.urlretrieve(checkpoint_url, checkpoint_path)\n",
                "    print(\"\u2705 Downloaded!\")\n",
                "else:\n",
                "    print(\"\u2705 Checkpoint already exists\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 8. Start Training\n",
                "\n",
                "import torch\n",
                "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
                "devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
                "print(f\"\ud83d\ude80 Training on {devices} {accelerator}(s)\")\n",
                "\n",
                "piper_src_path = os.path.abspath(\"piper_repo/src/python\")\n",
                "checkpoint_path = \"/content/drive/MyDrive/piper-model-training/checkpoints/epoch=2164-step=1355540.ckpt\"\n",
                "\n",
                "!PYTHONPATH=\"{piper_src_path}\" python -m piper_train \\\n",
                "  --dataset-dir /content/drive/MyDrive/piper-model-training/training_dir \\\n",
                "  --accelerator {accelerator} \\\n",
                "  --devices {devices} \\\n",
                "  --batch-size 16 \\\n",
                "  --validation-split 0.0 \\\n",
                "  --num-test-examples 0 \\\n",
                "  --max_epochs 10000 \\\n",
                "  --resume_from_checkpoint \"{checkpoint_path}\" \\\n",
                "  --checkpoint-epochs 1 \\\n",
                "  --quality medium \\\n",
                "  --precision 32"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        },
        "nbformat": 4,
        "nbformat_minor": 5
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
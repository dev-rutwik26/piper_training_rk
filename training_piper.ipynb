{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Piper TTS Training on Google Colab (Fixed)\n",
                "\n",
                "This notebook trains a Piper TTS model using a custom dataset on Google Drive.\n",
                "It includes fixes for:\n",
                "1.  **Dependencies**: Compiles `piper-phonemize` and `espeak-ng` from source to avoid Python 3.12 incompatibility.\n",
                "2.  **Permissions**: Builds in `/content` to avoid Google Drive `noexec` permission errors.\n",
                "3.  **PyTorch Lightning**: Compatible with Lightning 2.x via patches."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1. Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2. Install Python Dependencies (PyTorch < 2.6)\n",
                "# We strictly use PyTorch 2.5.1 to avoid the new 'weights_only=True' default in 2.6\n",
                "# which breaks loading legacy Piper checkpoints (pathlib issues).\n",
                "!pip install -q torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1\n",
                "!pip install -q lightning==2.4.0\n",
                "!pip install -q librosa<1 numba==0.62.1\n",
                "!pip install -q onnx onnxruntime tensorboard tensorboardX\n",
                "!pip install -q pysilero-vad>=2.1 pathvalidate>=3\n",
                "!pip install -q phonemizer Unidecode tqdm inflect matplotlib pandas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 3. The \"God Script\" (Build Dependencies from Source)\n",
                "# This script handles EVERYTHING related to building espeak-ng and piper-phonemize.\n",
                "# It avoids Google Drive permission issues by building in /content.\n",
                "\n",
                "import os\n",
                "import shutil\n",
                "import subprocess\n",
                "\n",
                "# Define build directories in /content (NOT Drive)\n",
                "BUILD_DIR = \"/content/build_temp\"\n",
                "if os.path.exists(BUILD_DIR):\n",
                "    shutil.rmtree(BUILD_DIR)\n",
                "os.makedirs(BUILD_DIR)\n",
                "\n",
                "print(\"üöÄ Starting God Script: Building dependencies in /content...\")\n",
                "\n",
                "# 1. Install System Dependencies\n",
                "print(\"üì¶ Installing system dependencies...\")\n",
                "!sudo apt-get update -qq\n",
                "!sudo apt-get install -y -qq build-essential cmake git autoconf automake libtool pkg-config\n",
                "\n",
                "# 2. Build & Install Rhasspy's espeak-ng Fork\n",
                "print(\"‚¨áÔ∏è Cloning rhasspy/espeak-ng...\")\n",
                "!cd {BUILD_DIR} && git clone https://github.com/rhasspy/espeak-ng.git\n",
                "\n",
                "print(\"üî® Building espeak-ng... (This takes ~2 mins)\")\n",
                "# We force standard paths to ensure linker finds them later\n",
                "!cd {BUILD_DIR}/espeak-ng && ./autogen.sh && ./configure --prefix=/usr --without-async --without-mbrola --without-sonic && make -j4 && sudo make install\n",
                "print(\"‚úÖ espeak-ng installed!\")\n",
                "\n",
                "# 3. Build piper-phonemize (The tricky part)\n",
                "print(\"‚¨áÔ∏è Cloning piper-phonemize...\")\n",
                "!cd {BUILD_DIR} && git clone https://github.com/rhasspy/piper-phonemize.git\n",
                "\n",
                "# Download ONNX Runtime (needed for headers)\n",
                "print(\"‚¨áÔ∏è Downloading ONNX Runtime...\")\n",
                "onnx_ver = \"1.14.1\"\n",
                "onnx_file = f\"onnxruntime-linux-x64-{onnx_ver}.tgz\"\n",
                "!cd {BUILD_DIR}/piper-phonemize && wget -q https://github.com/microsoft/onnxruntime/releases/download/v{onnx_ver}/{onnx_file}\n",
                "!cd {BUILD_DIR}/piper-phonemize && tar -xzf {onnx_file}\n",
                "!mkdir -p {BUILD_DIR}/piper-phonemize/lib/Linux-x86_64/onnxruntime/include\n",
                "!mkdir -p {BUILD_DIR}/piper-phonemize/lib/Linux-x86_64/onnxruntime/lib\n",
                "!cp -r {BUILD_DIR}/piper-phonemize/onnxruntime-linux-x64-{onnx_ver}/include/* {BUILD_DIR}/piper-phonemize/lib/Linux-x86_64/onnxruntime/include/\n",
                "!cp -r {BUILD_DIR}/piper-phonemize/onnxruntime-linux-x64-{onnx_ver}/lib/* {BUILD_DIR}/piper-phonemize/lib/Linux-x86_64/onnxruntime/lib/\n",
                "\n",
                "# Install Python build tools\n",
                "!pip install -q cython numpy pybind11 setuptools wheel\n",
                "\n",
                "# 4. Compile Python Extension\n",
                "print(\"üêç Compiling piper_phonemize python extension...\")\n",
                "# We MUST force the build to look in /usr/include for espeak-ng headers\n",
                "!cd {BUILD_DIR}/piper-phonemize && python setup.py build_ext --inplace\n",
                "!cd {BUILD_DIR}/piper-phonemize && python setup.py install\n",
                "\n",
                "print(\"‚úÖ piper-phonemize installed!\")\n",
                "\n",
                "# 5. Runtime Library Fix (Crucial!)\n",
                "print(\"üîß Applying Runtime Fixes...\")\n",
                "src_lib = f\"{BUILD_DIR}/piper-phonemize/lib/Linux-x86_64/onnxruntime/lib/libonnxruntime.so.{onnx_ver}\"\n",
                "!sudo cp {src_lib} /usr/lib/libonnxruntime.so.{onnx_ver}\n",
                "!sudo ln -fs /usr/lib/libonnxruntime.so.{onnx_ver} /usr/lib/libonnxruntime.so\n",
                "!sudo ldconfig\n",
                "\n",
                "print(\"üéâ GOD SCRIPT COMPLETE! Dependencies are ready.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 4. Clone Piper Repository & Patch Code\n",
                "import os\n",
                "\n",
                "# CLONE REPO\n",
                "if not os.path.exists(\"piper_repo\"):\n",
                "    !git clone https://github.com/rhasspy/piper.git piper_repo\n",
                "    print(\"‚úÖ Cloned piper repository\")\n",
                "else:\n",
                "    print(\"‚úÖ piper_repo already exists\")\n",
                "\n",
                "# --- PATCHING ---\n",
                "print(\"\\nü©π Applying PyTorch Lightning 2.x Patches...\")\n",
                "\n",
                "# 1. Fix __main__.py (Trainer compatibility)\n",
                "main_py_path = \"piper_repo/src/python/piper_train/__main__.py\"\n",
                "main_py_content = \"\"\"import argparse\n",
                "import json\n",
                "import logging\n",
                "from pathlib import Path\n",
                "\n",
                "import torch\n",
                "from pytorch_lightning import Trainer\n",
                "from pytorch_lightning.callbacks import ModelCheckpoint\n",
                "\n",
                "from .vits.lightning import VitsModel\n",
                "\n",
                "_LOGGER = logging.getLogger(__package__)\n",
                "\n",
                "\n",
                "def main():\n",
                "    logging.basicConfig(level=logging.DEBUG)\n",
                "\n",
                "    parser = argparse.ArgumentParser()\n",
                "    parser.add_argument(\n",
                "        \"--dataset-dir\", required=True, help=\"Path to pre-processed dataset directory\"\n",
                "    )\n",
                "    parser.add_argument(\n",
                "        \"--checkpoint-epochs\",\n",
                "        type=int,\n",
                "        help=\"Save checkpoint every N epochs (default: 1)\",\n",
                "    )\n",
                "    parser.add_argument(\n",
                "        \"--quality\",\n",
                "        default=\"medium\",\n",
                "        choices=(\"x-low\", \"medium\", \"high\"),\n",
                "        help=\"Quality/size of model (default: medium)\",\n",
                "    )\n",
                "    parser.add_argument(\n",
                "        \"--resume_from_single_speaker_checkpoint\",\n",
                "        help=\"For multi-speaker models only. Converts a single-speaker checkpoint to multi-speaker and resumes training\",\n",
                "    )\n",
                "    \n",
                "    # Manually add PL 2.x arguments that we use\n",
                "    parser.add_argument(\"--max_epochs\", type=int, default=1000)\n",
                "    parser.add_argument(\"--accelerator\", default=\"auto\")\n",
                "    parser.add_argument(\"--devices\", default=\"auto\")\n",
                "    parser.add_argument(\"--precision\", default=\"32-true\")\n",
                "    parser.add_argument(\"--default_root_dir\", type=str, default=None)\n",
                "    parser.add_argument(\"--resume_from_checkpoint\", type=str, default=None)\n",
                "\n",
                "    # Trainer.add_argparse_args(parser) # Removed in PL 2.0\n",
                "    VitsModel.add_model_specific_args(parser)\n",
                "    parser.add_argument(\"--seed\", type=int, default=1234)\n",
                "    args = parser.parse_args()\n",
                "    _LOGGER.debug(args)\n",
                "\n",
                "    args.dataset_dir = Path(args.dataset_dir)\n",
                "    if not args.default_root_dir:\n",
                "        args.default_root_dir = str(args.dataset_dir) # Must be string for Trainer explicitly\n",
                "    \n",
                "    torch.backends.cudnn.benchmark = True\n",
                "    torch.manual_seed(args.seed)\n",
                "\n",
                "    config_path = args.dataset_dir / \"config.json\"\n",
                "    dataset_path = args.dataset_dir / \"dataset.jsonl\"\n",
                "\n",
                "    with open(config_path, \"r\", encoding=\"utf-8\") as config_file:\n",
                "        config = json.load(config_file)\n",
                "        num_symbols = int(config[\"num_symbols\"])\n",
                "        num_speakers = int(config[\"num_speakers\"])\n",
                "        sample_rate = int(config[\"audio\"][\"sample_rate\"])\n",
                "\n",
                "\n",
                "    callbacks = []\n",
                "    if args.checkpoint_epochs is not None:\n",
                "        callbacks = [ModelCheckpoint(every_n_epochs=args.checkpoint_epochs)]\n",
                "        _LOGGER.debug(\n",
                "            \"Checkpoints will be saved every %s epoch(s)\", args.checkpoint_epochs\n",
                "        )\n",
                "\n",
                "    # Instantiate Trainer explicitly\n",
                "    trainer = Trainer(\n",
                "        max_epochs=args.max_epochs,\n",
                "        accelerator=args.accelerator,\n",
                "        devices=int(args.devices) if isinstance(args.devices, str) and args.devices.isdigit() else args.devices,\n",
                "        precision=args.precision,\n",
                "        default_root_dir=args.default_root_dir,\n",
                "        callbacks=callbacks\n",
                "    )\n",
                "\n",
                "    dict_args = vars(args)\n",
                "    if args.quality == \"x-low\":\n",
                "        dict_args[\"hidden_channels\"] = 96\n",
                "        dict_args[\"inter_channels\"] = 96\n",
                "        dict_args[\"filter_channels\"] = 384\n",
                "    elif args.quality == \"high\":\n",
                "        dict_args[\"resblock\"] = \"1\"\n",
                "        dict_args[\"resblock_kernel_sizes\"] = (3, 7, 11)\n",
                "        dict_args[\"resblock_dilation_sizes\"] = (\n",
                "            (1, 3, 5),\n",
                "            (1, 3, 5),\n",
                "            (1, 3, 5),\n",
                "        )\n",
                "        dict_args[\"upsample_rates\"] = (8, 8, 2, 2)\n",
                "        dict_args[\"upsample_initial_channel\"] = 512\n",
                "        dict_args[\"upsample_kernel_sizes\"] = (16, 16, 4, 4)\n",
                "\n",
                "    model = VitsModel(\n",
                "        num_symbols=num_symbols,\n",
                "        num_speakers=num_speakers,\n",
                "        sample_rate=sample_rate,\n",
                "        dataset=[dataset_path],\n",
                "        **dict_args,\n",
                "    )\n",
                "\n",
                "    if args.resume_from_single_speaker_checkpoint:\n",
                "        assert (\n",
                "            num_speakers > 1\n",
                "        ), \"--resume_from_single_speaker_checkpoint is only for multi-speaker models. Use --resume_from_checkpoint for single-speaker models.\"\n",
                "\n",
                "        # Load single-speaker checkpoint\n",
                "        _LOGGER.debug(\n",
                "            \"Resuming from single-speaker checkpoint: %s\",\n",
                "            args.resume_from_single_speaker_checkpoint,\n",
                "        )\n",
                "        model_single = VitsModel.load_from_checkpoint(\n",
                "            args.resume_from_single_speaker_checkpoint,\n",
                "            dataset=None,\n",
                "        )\n",
                "        g_dict = model_single.model_g.state_dict()\n",
                "        for key in list(g_dict.keys()):\n",
                "            # Remove keys that can't be copied over due to missing speaker embedding\n",
                "            if (\n",
                "                key.startswith(\"dec.cond\")\n",
                "                or key.startswith(\"dp.cond\")\n",
                "                or (\"enc.cond_layer\" in key)\n",
                "            ):\n",
                "                g_dict.pop(key, None)\n",
                "\n",
                "        # Copy over the multi-speaker model, excluding keys related to the\n",
                "        # speaker embedding (which is missing from the single-speaker model).\n",
                "        load_state_dict(model.model_g, g_dict)\n",
                "        load_state_dict(model.model_d, model_single.model_d.state_dict())\n",
                "        _LOGGER.info(\n",
                "            \"Successfully converted single-speaker checkpoint to multi-speaker\"\n",
                "        )\n",
                "\n",
                "    ckpt_path = args.resume_from_checkpoint\n",
                "    if args.resume_from_single_speaker_checkpoint:\n",
                "        ckpt_path = None # We manually loaded weights, start fresh\n",
                "\n",
                "    trainer.fit(model, ckpt_path=ckpt_path)\n",
                "\n",
                "\n",
                "def load_state_dict(model, saved_state_dict):\n",
                "    state_dict = model.state_dict()\n",
                "    new_state_dict = {}\n",
                "\n",
                "    for k, v in state_dict.items():\n",
                "        if k in saved_state_dict:\n",
                "            # Use saved value\n",
                "            new_state_dict[k] = saved_state_dict[k]\n",
                "        else:\n",
                "            # Use initialized value\n",
                "            _LOGGER.debug(\"%s is not in the checkpoint\", k)\n",
                "            new_state_dict[k] = v\n",
                "\n",
                "    model.load_state_dict(new_state_dict)\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()\n",
                "\"\"\"\n",
                "with open(main_py_path, \"w\") as f:\n",
                "    f.write(main_py_content)\n",
                "print(\"‚úÖ Fixed piper_train/__main__.py\")\n",
                "\n",
                "# 2. Fix lightning.py (Manual Optimization)\n",
                "lightning_py_path = \"piper_repo/src/python/piper_train/vits/lightning.py\"\n",
                "lightning_py_content = \"\"\"import logging\n",
                "from pathlib import Path\n",
                "from typing import List, Optional, Tuple, Union\n",
                "\n",
                "import pytorch_lightning as pl\n",
                "import torch\n",
                "from torch import autocast\n",
                "from torch.nn import functional as F\n",
                "from torch.utils.data import DataLoader, Dataset, random_split\n",
                "\n",
                "from .commons import slice_segments\n",
                "from .dataset import Batch, PiperDataset, UtteranceCollate\n",
                "from .losses import discriminator_loss, feature_loss, generator_loss, kl_loss\n",
                "from .mel_processing import mel_spectrogram_torch, spec_to_mel_torch\n",
                "from .models import MultiPeriodDiscriminator, SynthesizerTrn\n",
                "\n",
                "_LOGGER = logging.getLogger(\"vits.lightning\")\n",
                "\n",
                "\n",
                "class VitsModel(pl.LightningModule):\n",
                "    def __init__(\n",
                "        self,\n",
                "        num_symbols: int,\n",
                "        num_speakers: int,\n",
                "        # audio\n",
                "        resblock=\"2\",\n",
                "        resblock_kernel_sizes=(3, 5, 7),\n",
                "        resblock_dilation_sizes=(\n",
                "            (1, 2),\n",
                "            (2, 6),\n",
                "            (3, 12),\n",
                "        ),\n",
                "        upsample_rates=(8, 8, 4),\n",
                "        upsample_initial_channel=256,\n",
                "        upsample_kernel_sizes=(16, 16, 8),\n",
                "        # mel\n",
                "        filter_length: int = 1024,\n",
                "        hop_length: int = 256,\n",
                "        win_length: int = 1024,\n",
                "        mel_channels: int = 80,\n",
                "        sample_rate: int = 22050,\n",
                "        sample_bytes: int = 2,\n",
                "        channels: int = 1,\n",
                "        mel_fmin: float = 0.0,\n",
                "        mel_fmax: Optional[float] = None,\n",
                "        # model\n",
                "        inter_channels: int = 192,\n",
                "        hidden_channels: int = 192,\n",
                "        filter_channels: int = 768,\n",
                "        n_heads: int = 2,\n",
                "        n_layers: int = 6,\n",
                "        kernel_size: int = 3,\n",
                "        p_dropout: float = 0.1,\n",
                "        n_layers_q: int = 3,\n",
                "        use_spectral_norm: bool = False,\n",
                "        gin_channels: int = 0,\n",
                "        use_sdp: bool = True,\n",
                "        segment_size: int = 8192,\n",
                "        # training\n",
                "        dataset: Optional[List[Union[str, Path]]] = None,\n",
                "        learning_rate: float = 2e-4,\n",
                "        betas: Tuple[float, float] = (0.8, 0.99),\n",
                "        eps: float = 1e-9,\n",
                "        batch_size: int = 1,\n",
                "        lr_decay: float = 0.999875,\n",
                "        init_lr_ratio: float = 1.0,\n",
                "        warmup_epochs: int = 0,\n",
                "        c_mel: int = 45,\n",
                "        c_kl: float = 1.0,\n",
                "        grad_clip: Optional[float] = None,\n",
                "        num_workers: int = 1,\n",
                "        seed: int = 1234,\n",
                "        num_test_examples: int = 5,\n",
                "        validation_split: float = 0.1,\n",
                "        max_phoneme_ids: Optional[int] = None,\n",
                "        **kwargs,\n",
                "    ):\n",
                "        super().__init__()\n",
                "        self.save_hyperparameters()\n",
                "        \n",
                "        # Lightning 2.x requires manual optimization for multiple optimizers\n",
                "        self.automatic_optimization = False\n",
                "\n",
                "        if (self.hparams.num_speakers > 1) and (self.hparams.gin_channels <= 0):\n",
                "            # Default gin_channels for multi-speaker model\n",
                "            self.hparams.gin_channels = 512\n",
                "\n",
                "        # Set up models\n",
                "        self.model_g = SynthesizerTrn(\n",
                "            n_vocab=self.hparams.num_symbols,\n",
                "            spec_channels=self.hparams.filter_length // 2 + 1,\n",
                "            segment_size=self.hparams.segment_size // self.hparams.hop_length,\n",
                "            inter_channels=self.hparams.inter_channels,\n",
                "            hidden_channels=self.hparams.hidden_channels,\n",
                "            filter_channels=self.hparams.filter_channels,\n",
                "            n_heads=self.hparams.n_heads,\n",
                "            n_layers=self.hparams.n_layers,\n",
                "            kernel_size=self.hparams.kernel_size,\n",
                "            p_dropout=self.hparams.p_dropout,\n",
                "            resblock=self.hparams.resblock,\n",
                "            resblock_kernel_sizes=self.hparams.resblock_kernel_sizes,\n",
                "            resblock_dilation_sizes=self.hparams.resblock_dilation_sizes,\n",
                "            upsample_rates=self.hparams.upsample_rates,\n",
                "            upsample_initial_channel=self.hparams.upsample_initial_channel,\n",
                "            upsample_kernel_sizes=self.hparams.upsample_kernel_sizes,\n",
                "            n_speakers=self.hparams.num_speakers,\n",
                "            gin_channels=self.hparams.gin_channels,\n",
                "            use_sdp=self.hparams.use_sdp,\n",
                "        )\n",
                "        self.model_d = MultiPeriodDiscriminator(\n",
                "            use_spectral_norm=self.hparams.use_spectral_norm\n",
                "        )\n",
                "\n",
                "        # Dataset splits\n",
                "        self._train_dataset: Optional[Dataset] = None\n",
                "        self._val_dataset: Optional[Dataset] = None\n",
                "        self._test_dataset: Optional[Dataset] = None\n",
                "        self._load_datasets(validation_split, num_test_examples, max_phoneme_ids)\n",
                "\n",
                "        # State kept between training optimizers\n",
                "        self._y = None\n",
                "        self._y_hat = None\n",
                "\n",
                "    def _load_datasets(\n",
                "        self,\n",
                "        validation_split: float,\n",
                "        num_test_examples: int,\n",
                "        max_phoneme_ids: Optional[int] = None,\n",
                "    ):\n",
                "        if self.hparams.dataset is None:\n",
                "            _LOGGER.debug(\"No dataset to load\")\n",
                "            return\n",
                "\n",
                "        full_dataset = PiperDataset(\n",
                "            self.hparams.dataset, max_phoneme_ids=max_phoneme_ids\n",
                "        )\n",
                "        valid_set_size = int(len(full_dataset) * validation_split)\n",
                "        train_set_size = len(full_dataset) - valid_set_size - num_test_examples\n",
                "\n",
                "        self._train_dataset, self._test_dataset, self._val_dataset = random_split(\n",
                "            full_dataset, [train_set_size, num_test_examples, valid_set_size]\n",
                "        )\n",
                "\n",
                "    def forward(self, text, text_lengths, scales, sid=None):\n",
                "        noise_scale = scales[0]\n",
                "        length_scale = scales[1]\n",
                "        noise_scale_w = scales[2]\n",
                "        audio, *_ = self.model_g.infer(\n",
                "            text,\n",
                "            text_lengths,\n",
                "            noise_scale=noise_scale,\n",
                "            length_scale=length_scale,\n",
                "            noise_scale_w=noise_scale_w,\n",
                "            sid=sid,\n",
                "        )\n",
                "\n",
                "        return audio\n",
                "\n",
                "    def train_dataloader(self):\n",
                "        return DataLoader(\n",
                "            self._train_dataset,\n",
                "            collate_fn=UtteranceCollate(\n",
                "                is_multispeaker=self.hparams.num_speakers > 1,\n",
                "                segment_size=self.hparams.segment_size,\n",
                "            ),\n",
                "            num_workers=self.hparams.num_workers,\n",
                "            batch_size=self.hparams.batch_size,\n",
                "        )\n",
                "\n",
                "    def val_dataloader(self):\n",
                "        return DataLoader(\n",
                "            self._val_dataset,\n",
                "            collate_fn=UtteranceCollate(\n",
                "                is_multispeaker=self.hparams.num_speakers > 1,\n",
                "                segment_size=self.hparams.segment_size,\n",
                "            ),\n",
                "            num_workers=self.hparams.num_workers,\n",
                "            batch_size=self.hparams.batch_size,\n",
                "        )\n",
                "\n",
                "    def test_dataloader(self):\n",
                "        return DataLoader(\n",
                "            self._test_dataset,\n",
                "            collate_fn=UtteranceCollate(\n",
                "                is_multispeaker=self.hparams.num_speakers > 1,\n",
                "                segment_size=self.hparams.segment_size,\n",
                "            ),\n",
                "            num_workers=self.hparams.num_workers,\n",
                "            batch_size=self.hparams.batch_size,\n",
                "        )\n",
                "\n",
                "    def training_step(self, batch: Batch, batch_idx: int):\n",
                "        # Manual optimization for Lightning 2.x with multiple optimizers\n",
                "        opt_g, opt_d = self.optimizers()\n",
                "        \n",
                "        # Train Generator\n",
                "        loss_gen_all = self.training_step_g(batch)\n",
                "        opt_g.zero_grad()\n",
                "        self.manual_backward(loss_gen_all)\n",
                "        opt_g.step()\n",
                "        \n",
                "        # Train Discriminator\n",
                "        loss_disc_all = self.training_step_d(batch)\n",
                "        opt_d.zero_grad()\n",
                "        self.manual_backward(loss_disc_all)\n",
                "        opt_d.step()\n",
                "        \n",
                "        # Step learning rate schedulers\n",
                "        sch_g, sch_d = self.lr_schedulers()\n",
                "        if self.trainer.is_last_batch:\n",
                "             sch_g.step()\n",
                "             sch_d.step()\n",
                "\n",
                "\n",
                "    def training_step_g(self, batch: Batch):\n",
                "        x, x_lengths, y, _, spec, spec_lengths, speaker_ids = (\n",
                "            batch.phoneme_ids,\n",
                "            batch.phoneme_lengths,\n",
                "            batch.audios,\n",
                "            batch.audio_lengths,\n",
                "            batch.spectrograms,\n",
                "            batch.spectrogram_lengths,\n",
                "            batch.speaker_ids if batch.speaker_ids is not None else None,\n",
                "        )\n",
                "        (\n",
                "            y_hat,\n",
                "            l_length,\n",
                "            _attn,\n",
                "            ids_slice,\n",
                "            _x_mask,\n",
                "            z_mask,\n",
                "            (_z, z_p, m_p, logs_p, _m_q, logs_q),\n",
                "        ) = self.model_g(x, x_lengths, spec, spec_lengths, speaker_ids)\n",
                "        self._y_hat = y_hat\n",
                "\n",
                "        mel = spec_to_mel_torch(\n",
                "            spec,\n",
                "            self.hparams.filter_length,\n",
                "            self.hparams.mel_channels,\n",
                "            self.hparams.sample_rate,\n",
                "            self.hparams.mel_fmin,\n",
                "            self.hparams.mel_fmax,\n",
                "        )\n",
                "        y_mel = slice_segments(\n",
                "            mel,\n",
                "            ids_slice,\n",
                "            self.hparams.segment_size // self.hparams.hop_length,\n",
                "        )\n",
                "        y_hat_mel = mel_spectrogram_torch(\n",
                "            y_hat.squeeze(1),\n",
                "            self.hparams.filter_length,\n",
                "            self.hparams.mel_channels,\n",
                "            self.hparams.sample_rate,\n",
                "            self.hparams.hop_length,\n",
                "            self.hparams.win_length,\n",
                "            self.hparams.mel_fmin,\n",
                "            self.hparams.mel_fmax,\n",
                "        )\n",
                "        y = slice_segments(\n",
                "            y,\n",
                "            ids_slice * self.hparams.hop_length,\n",
                "            self.hparams.segment_size,\n",
                "        )  # slice\n",
                "\n",
                "        # Save for training_step_d\n",
                "        self._y = y\n",
                "\n",
                "        _y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = self.model_d(y, y_hat)\n",
                "\n",
                "        with autocast(self.device.type, enabled=False):\n",
                "            # Generator loss\n",
                "            loss_dur = torch.sum(l_length.float())\n",
                "            loss_mel = F.l1_loss(y_mel, y_hat_mel) * self.hparams.c_mel\n",
                "            loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * self.hparams.c_kl\n",
                "\n",
                "            loss_fm = feature_loss(fmap_r, fmap_g)\n",
                "            loss_gen, _losses_gen = generator_loss(y_d_hat_g)\n",
                "            loss_gen_all = loss_gen + loss_fm + loss_mel + loss_dur + loss_kl\n",
                "\n",
                "            self.log(\"loss_gen_all\", loss_gen_all, prog_bar=True)\n",
                "\n",
                "            return loss_gen_all\n",
                "\n",
                "    def training_step_d(self, batch: Batch):\n",
                "        # From training_step_g\n",
                "        y = self._y\n",
                "        y_hat = self._y_hat\n",
                "        y_d_hat_r, y_d_hat_g, _, _ = self.model_d(y, y_hat.detach())\n",
                "\n",
                "        with autocast(self.device.type, enabled=False):\n",
                "            # Discriminator\n",
                "            loss_disc, _losses_disc_r, _losses_disc_g = discriminator_loss(\n",
                "                y_d_hat_r, y_d_hat_g\n",
                "            )\n",
                "            loss_disc_all = loss_disc\n",
                "\n",
                "            self.log(\"loss_disc_all\", loss_disc_all, prog_bar=True)\n",
                "\n",
                "            return loss_disc_all\n",
                "\n",
                "    def validation_step(self, batch: Batch, batch_idx: int):\n",
                "        val_loss = self.training_step_g(batch) + self.training_step_d(batch)\n",
                "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
                "\n",
                "        # Generate audio examples\n",
                "        for utt_idx, test_utt in enumerate(self._test_dataset):\n",
                "            text = test_utt.phoneme_ids.unsqueeze(0).to(self.device)\n",
                "            text_lengths = torch.LongTensor([len(test_utt.phoneme_ids)]).to(self.device)\n",
                "            scales = [0.667, 1.0, 0.8]\n",
                "            sid = (\n",
                "                test_utt.speaker_id.to(self.device)\n",
                "                if test_utt.speaker_id is not None\n",
                "                else None\n",
                "            )\n",
                "            test_audio = self(text, text_lengths, scales, sid=sid).detach()\n",
                "\n",
                "            # Scale to make louder in [-1, 1]\n",
                "            test_audio = test_audio * (1.0 / max(0.01, abs(test_audio.max())))\n",
                "\n",
                "            tag = test_utt.text or str(utt_idx)\n",
                "            self.logger.experiment.add_audio(\n",
                "                tag, test_audio, sample_rate=self.hparams.sample_rate\n",
                "            )\n",
                "\n",
                "        return val_loss\n",
                "\n",
                "    def configure_optimizers(self):\n",
                "        optimizers = [\n",
                "            torch.optim.AdamW(\n",
                "                self.model_g.parameters(),\n",
                "                lr=self.hparams.learning_rate,\n",
                "                betas=self.hparams.betas,\n",
                "                eps=self.hparams.eps,\n",
                "            ),\n",
                "            torch.optim.AdamW(\n",
                "                self.model_d.parameters(),\n",
                "                lr=self.hparams.learning_rate,\n",
                "                betas=self.hparams.betas,\n",
                "                eps=self.hparams.eps,\n",
                "            ),\n",
                "        ]\n",
                "        schedulers = [\n",
                "            torch.optim.lr_scheduler.ExponentialLR(\n",
                "                optimizers[0], gamma=self.hparams.lr_decay\n",
                "            ),\n",
                "            torch.optim.lr_scheduler.ExponentialLR(\n",
                "                optimizers[1], gamma=self.hparams.lr_decay\n",
                "            ),\n",
                "        ]\n",
                "\n",
                "        return [\n",
                "            {\"optimizer\": optimizers[0], \"lr_scheduler\": schedulers[0]},\n",
                "            {\"optimizer\": optimizers[1], \"lr_scheduler\": schedulers[1]},\n",
                "        ]\n",
                "\n",
                "    @staticmethod\n",
                "    def add_model_specific_args(parent_parser):\n",
                "        parser = parent_parser.add_argument_group(\"VitsModel\")\n",
                "        parser.add_argument(\"--batch-size\", type=int, required=True)\n",
                "        parser.add_argument(\"--validation-split\", type=float, default=0.1)\n",
                "        parser.add_argument(\"--num-test-examples\", type=int, default=5)\n",
                "        parser.add_argument(\n",
                "            \"--max-phoneme-ids\",\n",
                "            type=int,\n",
                "            help=\"Exclude utterances with phoneme id lists longer than this\",\n",
                "        )\n",
                "        #\n",
                "        parser.add_argument(\"--hidden-channels\", type=int, default=192)\n",
                "        parser.add_argument(\"--inter-channels\", type=int, default=192)\n",
                "        parser.add_argument(\"--filter-channels\", type=int, default=768)\n",
                "        parser.add_argument(\"--n-layers\", type=int, default=6)\n",
                "        parser.add_argument(\"--n-heads\", type=int, default=2)\n",
                "        #\n",
                "        return parent_parser\n",
                "\"\"\"\n",
                "with open(lightning_py_path, \"w\") as f:\n",
                "    f.write(lightning_py_content)\n",
                "print(\"‚úÖ Fixed piper_train/vits/lightning.py\")\n",
                "\n",
                "# 3. Fix monotonic_align/__init__.py\n",
                "monotonic_init = \"piper_repo/src/python/piper_train/vits/monotonic_align/__init__.py\"\n",
                "if os.path.exists(monotonic_init):\n",
                "    with open(monotonic_init, \"r\") as f:\n",
                "        content = f.read()\n",
                "    content = content.replace(\"from .monotonic_align.core\", \"from .core\")\n",
                "    with open(monotonic_init, \"w\") as f:\n",
                "        f.write(content)\n",
                "    print(\"‚úÖ Fixed monotonic_align import\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 5. Build Monotonic Align (Required for VITS)\n",
                "import os\n",
                "import shutil\n",
                "\n",
                "piper_src_path = os.path.abspath(\"piper_repo/src/python\")\n",
                "monotonic_align_src = os.path.join(piper_src_path, \"piper_train/vits/monotonic_align\")\n",
                "\n",
                "# Build directly in place since we have full permissions usually\n",
                "# But to be safe and clean, use temp\n",
                "temp_build_dir = \"/content/monotonic_align_build\"\n",
                "if os.path.exists(temp_build_dir):\n",
                "    shutil.rmtree(temp_build_dir)\n",
                "os.makedirs(temp_build_dir, exist_ok=True)\n",
                "\n",
                "for filename in [\"core.pyx\", \"setup.py\"]:\n",
                "    shutil.copy(os.path.join(monotonic_align_src, filename), os.path.join(temp_build_dir, filename))\n",
                "\n",
                "%cd {temp_build_dir}\n",
                "print(\"üî® Building monotonic_align...\")\n",
                "!python setup.py build_ext --inplace\n",
                "\n",
                "import glob\n",
                "so_files = glob.glob(\"core*.so\")\n",
                "if so_files:\n",
                "    dest = os.path.join(monotonic_align_src, so_files[0])\n",
                "    shutil.copy(so_files[0], dest)\n",
                "    print(f\"‚úÖ Installed compiled extension to: {dest}\")\n",
                "else:\n",
                "    print(\"‚ùå Build failed, no .so file found\")\n",
                "\n",
                "%cd /content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 6. Run Preprocessing\n",
                "import os\n",
                "import sys\n",
                "\n",
                "piper_src_path = os.path.abspath(\"piper_repo/src/python\")\n",
                "DATASET_PATH = \"/content/drive/MyDrive/english\" # Ensure this matches your uploaded folder name\n",
                "OUTPUT_DIR = \"/content/drive/MyDrive/piper-model-training/training_dir\"\n",
                "\n",
                "print(f\"üìÇ Dataset: {DATASET_PATH}\")\n",
                "print(f\"üìÇ Output: {OUTPUT_DIR}\")\n",
                "\n",
                "# Add piper_phonemize to path if needed\n",
                "\n",
                "!PYTHONPATH=\"{piper_src_path}\" python -m piper_train.preprocess \\\n",
                "  --language en \\\n",
                "  --input-dir \"{DATASET_PATH}\" \\\n",
                "  --output-dir \"{OUTPUT_DIR}\" \\\n",
                "  --dataset-format ljspeech \\\n",
                "  --single-speaker \\\n",
                "  --sample-rate 22050\n",
                "\n",
                "print(\"\\n‚úÖ Preprocessing complete (if no errors above)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 7. Download Base Checkpoint\n",
                "import urllib.request\n",
                "\n",
                "os.makedirs(\"/content/drive/MyDrive/piper-model-training/checkpoints\", exist_ok=True)\n",
                "checkpoint_url = \"https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/en/en_US/lessac/medium/epoch%3D2164-step%3D1355540.ckpt\"\n",
                "checkpoint_path = \"/content/drive/MyDrive/piper-model-training/checkpoints/epoch=2164-step=1355540.ckpt\"\n",
                "\n",
                "if not os.path.exists(checkpoint_path):\n",
                "    print(\"üì• Downloading base checkpoint...\")\n",
                "    urllib.request.urlretrieve(checkpoint_url, checkpoint_path)\n",
                "    print(\"‚úÖ Downloaded!\")\n",
                "else:\n",
                "    print(\"‚úÖ Checkpoint already exists\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üöÄ Training on 1 cpu(s)\n",
                        "DEBUG:piper_train:Namespace(dataset_dir='scripts/training_dir', checkpoint_epochs=1, quality='medium', resume_from_single_speaker_checkpoint=None, max_epochs=10000, accelerator='cpu', devices='1', precision='32', default_root_dir=None, resume_from_checkpoint='scripts/checkpoints/epoch=2164-step=1355540.ckpt', batch_size=16, validation_split=0.0, num_test_examples=0, num_workers=1, max_phoneme_ids=None, hidden_channels=192, inter_channels=192, filter_channels=768, n_layers=6, n_heads=2, seed=1234)\n",
                        "DEBUG:piper_train:Checkpoints will be saved every 1 epoch(s)\n",
                        "GPU available: False, used: False\n",
                        "TPU available: False, using: 0 TPU cores\n",
                        "/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
                        "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
                        "DEBUG:vits.dataset:Loading dataset: scripts/training_dir/dataset.jsonl\n",
                        "2026-01-23 22:36:30.265766: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
                        "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                        "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
                        "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
                        "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
                        "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
                        "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
                        "/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
                        "  if not hasattr(np, \"object\"):\n",
                        "Restoring states from the checkpoint path at scripts/checkpoints/epoch=2164-step=1355540.ckpt\n",
                        "DEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/checkpoints/epoch=2164-step=1355540.ckpt\n",
                        "Lightning automatically upgraded your loaded checkpoint from v1.9.0 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint scripts/checkpoints/epoch=2164-step=1355540.ckpt`\n",
                        "/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:566: The dirpath has changed from '/home/hansenm/larynx2/local/en-us/lessac/medium/lightning_logs/version_2/checkpoints' to 'scripts/training_dir/lightning_logs/version_0/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
                        "‚îè‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
                        "‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mType                    \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m‚îÉ\n",
                        "‚î°‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
                        "‚îÇ\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m‚îÇ model_g ‚îÇ SynthesizerTrn           ‚îÇ 23.7 M ‚îÇ train ‚îÇ     0 ‚îÇ\n",
                        "‚îÇ\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m‚îÇ model_d ‚îÇ MultiPeriodDiscriminator ‚îÇ 46.7 M ‚îÇ train ‚îÇ     0 ‚îÇ\n",
                        "‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                        "\u001b[1mTrainable params\u001b[0m: 70.4 M                                                        \n",
                        "\u001b[1mNon-trainable params\u001b[0m: 0                                                         \n",
                        "\u001b[1mTotal params\u001b[0m: 70.4 M                                                            \n",
                        "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 281                                     \n",
                        "\u001b[1mModules in train mode\u001b[0m: 504                                                      \n",
                        "\u001b[1mModules in eval mode\u001b[0m: 0                                                         \n",
                        "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                  \n",
                        "DEBUG:fsspec.local:open file: /Users/rutwik/piper-model-training/scripts/training_dir/lightning_logs/version_0/hparams.yaml\n",
                        "Restored all states from the checkpoint at scripts/checkpoints/epoch=2164-step=1355540.ckpt\n",
                        "\u001b[2K/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorc\n",
                        "h_lightning/trainer/connectors/data_connector.py:429: Consider setting \n",
                        "`persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker \n",
                        "initialization.\n",
                        "\u001b[2K/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorc\n",
                        "h_lightning/utilities/data.py:106: Total length of `DataLoader` across ranks is \n",
                        "zero. Please make sure this was your intention.\n",
                        "\u001b[2K/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorc\n",
                        "h_lightning/trainer/connectors/data_connector.py:429: Consider setting \n",
                        "`persistent_workers=True` in 'train_dataloader' to speed up the dataloader \n",
                        "worker initialization.\n",
                        "\u001b[2K/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorc\n",
                        "h_lightning/loops/fit_loop.py:317: The number of training batches (2) is smaller\n",
                        "than the logging interval Trainer(log_every_n_steps=50). Set a lower value for \n",
                        "log_every_n_steps if you want to see logs for the training epoch.\n",
                        "\u001b[2KEpoch 2165/9999 \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m 0/2 \u001b[2m0:00:01 ‚Ä¢ -:--:--\u001b[0m \u001b[2;4m0.00it/s\u001b[0m \n",
                        "\u001b[?25hTraceback (most recent call last):\n",
                        "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
                        "  File \"<frozen runpy>\", line 88, in _run_code\n",
                        "  File \"/Users/rutwik/piper-model-training/piper_repo/src/python/piper_train/__main__.py\", line 174, in <module>\n",
                        "    main()\n",
                        "  File \"/Users/rutwik/piper-model-training/piper_repo/src/python/piper_train/__main__.py\", line 151, in main\n",
                        "    trainer.fit(model, ckpt_path=ckpt_path)\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 584, in fit\n",
                        "    call._call_and_handle_interrupt(\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py\", line 49, in _call_and_handle_interrupt\n",
                        "    return trainer_fn(*args, **kwargs)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 630, in _fit_impl\n",
                        "    self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 1079, in _run\n",
                        "    results = self._run_stage()\n",
                        "              ^^^^^^^^^^^^^^^^^\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 1123, in _run_stage\n",
                        "    self.fit_loop.run()\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py\", line 217, in run\n",
                        "    self.advance()\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py\", line 465, in advance\n",
                        "    self.epoch_loop.run(self._data_fetcher)\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 153, in run\n",
                        "    self.advance(data_fetcher)\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 311, in advance\n",
                        "    batch, _, __ = next(data_fetcher)\n",
                        "                   ^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorch_lightning/loops/fetchers.py\", line 134, in __next__\n",
                        "    batch = super().__next__()\n",
                        "            ^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorch_lightning/loops/fetchers.py\", line 61, in __next__\n",
                        "    batch = next(self.iterator)\n",
                        "            ^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorch_lightning/utilities/combined_loader.py\", line 341, in __next__\n",
                        "    out = next(self._iterator)\n",
                        "          ^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/pytorch_lightning/utilities/combined_loader.py\", line 78, in __next__\n",
                        "    out[i] = next(self.iterators[i])\n",
                        "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
                        "    data = self._next_data()\n",
                        "           ^^^^^^^^^^^^^^^^^\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1346, in _next_data\n",
                        "    return self._process_data(data)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1372, in _process_data\n",
                        "    data.reraise()\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/torch/_utils.py\", line 722, in reraise\n",
                        "    raise exception\n",
                        "FileNotFoundError: Caught FileNotFoundError in DataLoader worker process 0.\n",
                        "Original Traceback (most recent call last):\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n",
                        "    data = fetcher.fetch(index)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
                        "    data = self.dataset.__getitems__(possibly_batched_index)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/torch/utils/data/dataset.py\", line 399, in __getitems__\n",
                        "    return [self.dataset[self.indices[idx]] for idx in indices]\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/torch/utils/data/dataset.py\", line 399, in <listcomp>\n",
                        "    return [self.dataset[self.indices[idx]] for idx in indices]\n",
                        "            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/Users/rutwik/piper-model-training/piper_repo/src/python/piper_train/vits/dataset.py\", line 80, in __getitem__\n",
                        "    audio_norm=torch.load(utt.audio_norm_path),\n",
                        "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/torch/serialization.py\", line 998, in load\n",
                        "    with _open_file_like(f, 'rb') as opened_file:\n",
                        "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/torch/serialization.py\", line 445, in _open_file_like\n",
                        "    return _open_file(name_or_buffer, mode)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/Users/rutwik/miniconda3/envs/voice_training/lib/python3.11/site-packages/torch/serialization.py\", line 426, in __init__\n",
                        "    super().__init__(open(name, mode))\n",
                        "                     ^^^^^^^^^^^^^^^^\n",
                        "FileNotFoundError: [Errno 2] No such file or directory: 'training_dir/cache/22050/a89d9241cc5038be5fc606aedeeb1635d4013efa786ee3a4ab17246257bfcf65.pt'\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# @title 8. Start Training\n",
                "import os\n",
                "import torch\n",
                "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
                "devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
                "print(f\"üöÄ Training on {devices} {accelerator}(s)\")\n",
                "\n",
                "piper_src_path = os.path.abspath(\"piper_repo/src/python\")\n",
                "# FIXED PATH: Removed duplicate 'piper-model-training/'\n",
                "checkpoint_path = \"scripts/checkpoints/epoch=2164-step=1355540.ckpt\"\n",
                "\n",
                "!PYTHONPATH=\"{piper_src_path}\" python -m piper_train \\\n",
                "  --dataset-dir scripts/training_dir \\\n",
                "  --accelerator {accelerator} \\\n",
                "  --devices {devices} \\\n",
                "  --batch-size 16 \\\n",
                "  --validation-split 0.0 \\\n",
                "  --num-test-examples 0 \\\n",
                "  --max_epochs 10000 \\\n",
                "  --resume_from_checkpoint \"{checkpoint_path}\" \\\n",
                "  --checkpoint-epochs 1 \\\n",
                "  --quality medium \\\n",
                "  --precision 32"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ceb5a0b2",
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext tensorboard\n",
                "%tensorboard --logdir {training_dir_ruru}/lightning_logs"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "voice_training",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.13"
        },
        "nbformat": 4,
        "nbformat_minor": 5
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
